[
  {
    "path": "posts/2024-12-27-a-basic-tool-calling-agent/",
    "title": "A basic tool calling agent",
    "description": "Use Nvidia NIMs and Langchain to create an agent that uses tools to find you a coffee shop",
    "author": [
      {
        "name": "Sean Lopp",
        "url": {}
      }
    ],
    "date": "2024-12-27",
    "categories": [],
    "contents": "\nToday I set out to build a simple AI agent based on this excellent example.\nWhile the example in that post uses LangGraph to compose a series of agents, I ended up taking a slightly different approach.\nI wrote a single “agent” that is invoked multiple times. The agent is responsible for deciding whether its prompt includes enough information to answer the user’s question. If it does, the agent responds. If not, the agent invokes a LangChain “tool”.\nIn this toy example, the prompt asks the agent to recommend a coffee shop near an address. The tools available are two “mock” functions: - one that returns coffee shops - one that returns coffee shop reviews\nWhile the example uses mock functions, I think it would be possible to use the Google Maps and Search APIs to bring this to life.\nThe sample code is available in this gist and at the bottom of this post.\nA follow up is to more closely follow the pattern in the original post, breaking the steps out into separate agents with fewer responsibilities and storing the intermediates in graph state instead of in a single mega-prompt.\nHere is a sample callstack for the address “1 E 161st St, Bronx, NY 10451”\nCurrent Iteration: 1 \n\n## Tools Called: ['search_coffee_shops']\n\nCurrent Iteration: 2 \n\n## Tools Called: ['coffee_shop_review']\n\nCurrent Iteration: 3 \n\n## Tools Called: ['coffee_shop_review']\n\nRECOMMENDING...... Based on the reviews, I recommend 'RoastBean' \nbecause it has a positive review praising its friendly barista, \nrich and smooth espresso, and modern atmosphere.\nThe fictional reviews are fairly fun as well (also generated by a helper LLM):\n{'CremaRoast': \"I'm extremely disappointed with my recent visit to CremaRoast,\nlocated near 1 E 161st St in the Bronx. As a coffee aficionado, I was excited to\ntry out this shop, but unfortunately, it fell short of my expectations in nearly\nevery aspect. The atmosphere was lacking, with a drab and uninviting decor that\nmade me feel like I was in a bland, corporate office rather than a cozy coffee\nshop. The tables were cramped and uncomfortable, and the background music was\noverpowering and annoying. But the real letdown was the coffee itself. I ordered\na cappuccino, which was over-extracted and tasted more like burnt coffee than\nthe rich, velvety drink I was craving. The milk was also not steamed properly,\nresulting in a lukewarm and unappetizing texture. To make matters worse, the\nbarista seemed completely uninterested in my experience, barely acknowledging my\npresence and not even bothering to ask how my drink was. The prices were also\npretty steep, especially considering the subpar quality of the coffee. Overall,\nI would not recommend CremaRoast to anyone looking for a good cup of coffee in\nthe Bronx. With so many other excellent coffee shops in the area, it's just not\nworth the visit. Rating: 1/5 stars.\"}\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\nfrom typing_extensions import TypedDict\nfrom typing import List, Mapping\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain.tools import tool\nimport random\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers.openai_tools import PydanticToolsParser\n\n\nAPI_KEY = \"your-api-key\"\n\nllm = ChatNVIDIA(api_key=API_KEY, model=\"meta/llama-3.3-70b-instruct\")\n\n\n@tool\ndef search_coffee_shops(address: str, n: int) -> List[Mapping[str, float]]:\n    \"\"\"Provides a list of length n where each element of the list is the name and distance of a coffee shop near the input address\"\"\"\n    if n > 3:\n        raise ValueError(\"Agent asked for too many coffee shops in search result\")\n    shops = []\n    for i in range(1, n):\n        shops.append(get_shop(address, i))\n    return shops\n\n\ndef get_shop(address: str, result_index: int) -> Mapping[str, float]:\n    \"\"\"Mock utility that looks up coffee shops by address\"\"\"\n    # TODO implement this tool using the google maps API\n    shop_name_eles: List[str] = random.choices(\n        [\"Coffee\", \"Roast\", \"Bean\", \"Vanilla\", \"Mocha\", \"Grande\", \"Crema\", \"Taste\"], k=2\n    )\n    shop_name: str = \"\".join(shop_name_eles)\n    return {shop_name: random.gammavariate(1, 2)}\n\n\nclass FictionalReviewWriter(BaseModel):\n    \"\"\"Writes fictional reviews\"\"\"\n\n    review: str = Field(description=\"Review for a fictional coffee shop\")\n\n\nreview_writer = llm.with_structured_output(FictionalReviewWriter)\n\n\n@tool\ndef coffee_shop_review(shop_name: str, address: str) -> Mapping[str, str]:\n    \"\"\"Given the name of a coffee shop and a nearby address, returns a review of the shop\"\"\"\n    # TODO implement this tool using the google search API\n    sentiment = random.choice([\"positive\", \"negative\", \"neutral\"])\n    prompt = f\"\"\" Write a fictional review of a coffee shop named {shop_name} near the address {address} with overall {sentiment} sentiment\"\"\"\n    review = review_writer.invoke(prompt)\n    return {shop_name: review.review}\n\n\nclass LLMCoffeeState(TypedDict):\n    \"\"\"\n    Represents the state between calls to the LLM as it solves the user's question, potentially including tool invocations\n    \"\"\"\n\n    shops: List[Mapping[str, float]]\n    reviews: List[Mapping[str, str]]\n\n\nsystem_prompt = \"\"\"\n    You are a bot designed to find coffee shops near a supplied address and then recommend one of those shops based on shop reviews. Include a reason along with the name of the recommended shop.\n    In order to accomplish this task you have access to two tools. \n    The first tool search_coffee_shops will give a list of shops and distances. Call this tool if no shops are listed under SHOPS. Never call this tool with n > 3.\n    The second tool coffee_shop_review will supply a review. Call this tool with the name of each shop until you have one review for each candidate coffee shop.\n    Once you have a list of shops, and a review of each shop, reply with a recommendation and reason. Do not make further tool calls.\n    \n    SHOPS\n    {shops}\n    SHOP REVIEWS\n    {reviews} \n\"\"\"\n\ncore_agent = llm.bind_tools([search_coffee_shops, coffee_shop_review])\n\n\ndef main(address: str):\n    \"\"\"Invoke the agent to find a coffee shop near the supplied address\"\"\"\n\n    state = LLMCoffeeState(shops=[], reviews=[])\n\n    calls = 0\n\n    while calls <= 10:\n        # update prompt\n        prompt = ChatPromptTemplate.from_messages(\n            [\n                (\"system\", system_prompt),\n                (\"human\", \"I want a coffee shop recommendation for {address}\"),\n            ]\n        )\n\n        # Uncomment to see current prompt\n        # prompt_value = prompt.invoke({\n        #     \"address\": address,\n        #     \"shops\": str(state[\"shops\"]),\n        #     \"reviews\": str(state[\"reviews\"])\n        # })\n        # print(prompt_value)\n\n        agent = prompt | core_agent\n\n        result = agent.invoke(\n            {\n                \"address\": address,\n                \"shops\": str(state[\"shops\"]),\n                \"reviews\": str(state[\"reviews\"]),\n            }\n        )\n\n        if result.content != \"\":\n            print(f\"RECOMMENDING...... {result.content}\")\n            return\n\n        if len(result.tool_calls):\n            tools_called = []\n            for tool_call in result.tool_calls:\n                tool_name = tool_call[\"name\"].lower()\n                tools_called.append(tool_name)\n                selected_tool = {\n                    \"coffee_shop_review\": coffee_shop_review,\n                    \"search_coffee_shops\": search_coffee_shops,\n                }[tool_name]\n                tool_msg = selected_tool.invoke(tool_call)\n                if tool_name == \"search_coffee_shops\":\n                    state[\"shops\"] = eval(tool_msg.content)\n\n                if tool_name == \"coffee_shop_review\":\n                    state[\"reviews\"].append(eval(tool_msg.content))\n\n        calls += 1\n        print(f\"\"\"\nCurrent Iteration: {calls}\n___________________________\nTools Called: {tools_called}\n---------------------------\n            \"\"\")\n\n\nmain(\"1 E 161st St, Bronx, NY 10451\")\n\n\n\n",
    "preview": {},
    "last_modified": "2024-12-27T16:32:02-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-26-a-rag-to-chat-with-your-favorite-blogger/",
    "title": "A RAG to chat with your favorite blogger",
    "description": "Use Nvidia NIMs and Langchain to create a chatbot based on knowledge from your favorite blog",
    "author": [
      {
        "name": "Sean Lopp",
        "url": {}
      }
    ],
    "date": "2024-12-26",
    "categories": [],
    "contents": "\nOne of my goals this Christmas break was to explore LLMs in a bit more detail. One place I started was with the Torch, by recreating the R for Torch getting started guide but in Python which gave me my first exposure to speeding up a nueral network on a GPU using Modal. I also read “how to code chatgpt from scratch” on the RStudio AI blog. Neither of these were super helpful besides building a bit of a mental model.\nHowever, in the process I discovered this excellent post on how to create a chatbot using RAG with a custom document store.\nI decided to copy this code to build a chatbot that would learn from my favorite blog, theradavist.com. In general, this had two parts:\nWrite some Python code to download a corpus of texts from theradavist.com. Create a vector database with embeddings from this corpus using Nvidia NIM API calls to do the embeddings.\nCreate the chatbot that answers questions following this process:\nSearch for the most relevant documents based on the user’s question, using similarity search in the vector DB from step 1\nPass in the question, along with the relevant documents found in the prior step, to a LLM to get a response (using Llama via Nvidia NIM as the foundational LLM)\nHere is example usage and output:\n python chat_with_radavist.py  --text \"what is the best material to build a hardtail  out of?\"\n{'answer': 'According to the text, the author thinks that titanium is the best '\n           'material for a hardtail mountain bike. They mention that it allows '\n           'the bike to \"snap through tight singletrack, soften those '\n           'brake-bump riddled turns, eat chunderous fall-line for dinner, and '\n           'at the end of a long ride, won’t leave your back and wrists '\n           'wrecked.\" They also mention that their rigid 29+ desert touring '\n           'bike is made of titanium and that it \"soaks up rugged jeep roads '\n           'like a piece of cornbread atop a bowl of chili.\"',\n \n 'source_documents': [Document(metadata={'source': 'radavist_posts/moots-womble-29er-review.md'}, page_content='On a hardtail, this means it will snap through tight ...,\n                      Document(metadata={'source': 'radavist_posts/moots-womble-29er-review.md'}, page_content='On a hardtail, this means it will snap through tight ...!'),\n                      Document(metadata={'source': 'radavist_posts/wood-is-good-twmpa-cycles-gr1-gravel-bike-review.md'}, page_content='“I’ll preface this by saying ...,\n                      Document(metadata={'source': 'radavist_posts/wood-is-good-twmpa-cycles-gr1-gravel-bike-review.md'}, page_content='“I’ll preface this by saying ...\nThe full sequence of steps to reproduce would be:\nGet a Nivida NIM API Key from https://www.nvidia.com/en-us/ai/\nSetup a Python environment with roughly these dependencies:\nunstructured\nMarkdown\nlangchain-nvidia-ai-endpoints\nlangchain-community\nlangchain-chroma\nbs4\nRun these commands, which will download a corpus of text and then build the embeddings before doing Q&A\npython get_data.py \npython chat_with_radavist.py --rebuild  --text \"what is the best material to build a hardtail  out of?\"\nCode details\nFull source code can be found in this gist. Warning: This code is definitely “first draft” state, and could benefit from refactoring.\nThe general idea of get_data is to:\nparse the sitemap for theradavist\ndownload a bunch of html posts from the site into markdown\nThis code is fairly specific to theradavist’s site structure, but similar code could be written for any blog. Please be polite when programmatically bashing a blog. In particular, theradavist has a sitemap per year, so the code first downloads all URLs for a certain year. Then the code attempts to parse each of those URLs, excluding files that aren’t blogposts, and saves the resulting markdown files to disk. The process is done in parallel, with a bunch of error handling to ensure one unparsable page doesn’t stop everything.\nThe code in chat_with_radavist is almost a 1:1 copy of the blogpost noted earlier, swapping out Nvidia NIMs for OpenAI.\n\n\n\n",
    "preview": {},
    "last_modified": "2024-12-26T13:38:21-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-11-16-bike-check-2024/",
    "title": "Bike Check 2024",
    "description": "Talking about bikes",
    "author": [
      {
        "name": "Sean Lopp",
        "url": {}
      }
    ],
    "date": "2024-11-16",
    "categories": [],
    "contents": "\nThe brief history of my time riding bikes:\nAge 7ish? First bike was a blue Trek mountain bike from a garage sale. It was too big, and then very quickly too small.\nAge 9ish? Second bike was a purple Trek mountain bike. 12 speed. Super cool bike, great for racing circles at Cub Scout camp outs.\nAge 12ish Third bike was a Trek mountain bike. First brand new bike. First bike with front coil suspension (still rim brakes). Rode with cousins on real mountain bike trails. Lots of loops around the block with Dad. Rode my first 50 miler on this bike to get my cycling merit badge (from Keystone to Breckenridge and back via Frisco). This thing shredded.\nAge 15ish Fourth bike was my first 29er, some sort of Raleigh. Disc brakes (still coil shock and 3x upfront). I finally stopped growing, and this bike was in my stable the longest of any bike (so far), not sold until summer 2022 at age 28… thats 13 years! This bike’s most notable accomplishment was being used as a college commuter where it was stored outside for 3 years and then never. maintained. again.\nAge 17ish Fifth bike was a Raleigh road bike. Also kept a long time (until at least summer of 2018). Most notable for riding hot laps around the neighborhood in high school, and doing some bigger rides in Summit County. First bike with clip pedals (added in 2013) and first bike to take me to the top of a 14tner.\nRiding bikes was pretty much a constant from about 2nd grade until I graduated college. There was a brief dark period after college where I rode very little (2015-2017). I picked things back up in 2018 when I went on a spending spree:\n2018 (now age 24) Sixth bike was a Salsa Warbird. First Strava bike. First 2000+ mile bike (though who knows about the earlier ones). First bike where I started to do my own maintenance (and this bike has the scars… I scrapped the inner chainstay before I knew how to take a wheel on and off, both water bottle cages are held on by zip ties since I stripped the bolts, chain, chain ring, BB, cassette have all been replaced, and the right hood doesn’t really stay on after a few crashes and an Apex shifter replacement).\n2019 Seventh bike was a Trek Emonda SL6 Pro. This bike was bought to chase Strava segments, especially hill climbs. This bike holds the records for two of my bucket list rides (Golden to top of Evans in 2020, Triple Bypass in 2021).\n2022 Eighth bike was a Surly Wednesday (quasi steel fat bike, rigid, with 26x3.8 tires). I would say this purchase started my “bike enlightment” phase, which was (is) a period where I started consuming excessive amounts of bike media: The Radavist, Seth’s Bike Hacks, Bikepacking.com, The Ride with Ben Delaney, etc. The Wednesday was also my first foray back into mountain biking, motivated by (a) disdain for roads (b) a move to Evergreen where gravel was less available. In 2020/2021s I had tried an indoor trainer and hated it. The Wednesday became my first foray into biking year round (aside from college commuting). For awhile I went full on “anti-establishment”: forgoing lycra, riding without a Garmin or Strava. Winter 2022-summer 2023 it was run single speed. The bike is currently very close to its original form (geared, flat bar) but in the summer it serves duty as a Dad bike with jones bars and a seat.\nSummer 2023 Ninth bike was a Surly Karate Monkey. First modern hard tail, first air suspension fork. First dropper. First 27in+ bike. This bike was a ton of fun, but it was also heavy AF. I bought it to both try and level up my mountain biking skills (this did not work) and try to do longer mixed rides instead of driving the Surly Wednesday to the trail heads (this also did not really work). However, this bike did shred, especially at Abasin which is probably my most “enduro” descent.\nWinter 2023 Tenth bike was the dream bike a Moots Womble. A 29er with SRAM GRX, downcountry geo, and no dropper. I traded cash + the Surly KM for it. Why was this the dream bike? I had my eyes on Moots for a long time (honestly just intrigued by the price tag) and The Radavist told me that the Womble was the ultimate hardtail experience and the ultimate application of Ti (the ultimate bike material). This used Womble was about 50% of a new one. Was any of it true? The bike is light. It is also very fun, and the geo feels very intuitive both on and off trail It is not quite as capable as the dropper-equipped 27.5+ KM at the Abasin descent, but it is close to the perfect bike for riding neighborhood roads to the beaver brook watershed trail (a roughcut blue-ish trail that is do-able on a gravel bike but more fun on a hardtail). When I originally bought it I had wanted a n=1 quiver killer. Since then, I’ve learned that even a light hardtail really can’t compete with a dropbar bike for road/gravel - especially if you want to go fast. Do I want to go fast? TBD\n2024 No bike purchases (yet)! This year is the first time I’ve really tried to ride my bike every day. With 6 weeks left in the year I’m at about 2,750 miles ridden, and 290 hours. Short of my New Year’s goal of 1 hour per day, but still my most rides/hours on a bike ever! (My mileage is less than my 2020 PR of 2950 miles… but perhaps surpassable in 6 weeks time). Over the course of this year the pendulum has swung back a bit from “alt cycling” Sean… I now ride with my Garmin, with a power meter, and I did 3 drop bar rides this year that were all excellent and even in Spandex! Other highlights from this year include: learning how to track stand, a trip to New Mexico with Mootsy in tow, and a very memorable ride around Keystone where I fell and was fully submerged in a mountain creek.\nSo what is next?\nI would like to do more events in 2025, with my eyes on Old Man Winter, FoCo Fondo, and perhaps one of the Rambler endurance XC MTB events. I am trying to decide if I want to find group rides/biking friends … as almost all of my cycling journey has been solo (apart from my Dad!).\nA few other musings…\nI have definitely improved as a mechanic. I can now pretty reliably swap chains, brake pads, bars, stems, cassettes, and tubeless tires. I can even straighten derailer hangers and do a decent job tuning gears. In 2024 I did my first 3 attempts a brake bleeds with some succes. I’ve also successfully removed and re-installed the cranks and BB on Wednesday. I don’t really understand wheel truing, or want to touch bearings (the bearings in Wednesday’s headset are quasi-exploded from the time I accidentally removed the fork).\nI’ve gone through a LOT of variations on how to carry stuff on the bike. Handlebar bags, frame half bags, top tube bento box, jersey pockets, seat bags, and fanny pack. I don’t really know. I prefer the aesthetic of a bike without bags. I also prefer not carrying things on my back. What to do?\nI have also gone through a lot of variations on nutrition. I am pretty happy where I landed recently, which is riding with water bottles filled with drink mix. I used to do a lot of on-bike food (gummy worms) and during my alt-cycling phase I did a lot of “stop and eat”. I like the current setup because it has prevented bonks while still allowing a high ratio of moving time to ride time.\nWhat is the point of this post? I don’t know. To document my cycling vibe at the current moment and maybe laugh at how that vibe has changed over time (mostly due to YouTube recommendations).\nSo the bike check is… current stable:\n\n\nknitr::include_graphics(\"images/womble.jpg\")\n\n\n\n\n\nknitr::include_graphics(\"images/wednesday.jpg\")\n\n\n\n\n\nknitr::include_graphics(\"images/warbird.jpg\")\n\n\n\n\n\nknitr::include_graphics(\"images/emonda.jpg\")\n\n\n\n\n\n\n",
    "preview": "posts/2024-11-16-bike-check-2024/images/womble.jpg",
    "last_modified": "2024-11-16T16:16:18-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-07-from-the-behemoth/",
    "title": "From the Behemoth",
    "description": "Leaving Google",
    "author": [
      {
        "name": "Sean Lopp",
        "url": {}
      }
    ],
    "date": "2022-10-07",
    "categories": [],
    "contents": "\nWhen I left RStudio a year ago to join Google Cloud my goal was to get exposure to other ways of being a sales engineer. This post recaps what I have learned and why I’m excited to be leaving Google for a sales engineering role at a new start up, Elementl.\nI think it might be helpful to have a brief timeline of the past year:\nOctober-Nov: Ramping within Looker, which was still mostly independent.\nNov-Dec: Running the Looker SE process.\nJan: Re-org. Looker integrated with GCP, I land in a SMB COE.\nJan-Mar: Running the Looker SE process, but with new team.\nApr-June: Paternity leave.\nJune - Oct: Moved to enterprise GCP as a regional Looker specialist. Mostly running the GCP SE process.\nOutside Work\nThe biggest part of 2022 was becoming a new father. I am grateful to Google and my managers for allowing me to take 12 weeks of leave. I don’t like to post photos of my daughter publicly, so instead:\n\n\n\nThe second biggest part of 2022 was re-discovering mountain biking, which led to me selling my old college mountain bike (codename: “Charlie”), and buying a ridiculous Surly Wednesday. So slow. So fun.\nLooker Lessons\nLooker ran an incredibly tight sales team that was a great example of value oriented selling. They also built ramp, tools, and processes that executed their strategy. The Looker process included:\nInitial discovery call and demo. AEs led discovery using a value-driven markitecture slide.\nThe demo told a story using realistic data.\nCustomer PoCs would be done with a real Looker instance and followed a highly structured process incl:\nTrial kickoff where a mutual evaluation plan was scoped and co-written in a doc.\nCo-dev sessions where a Looker SE would write code and build dashboards alongside the prospect Progress was tracked in the evaluation plan.\nOnce the deal was closed the PoC would become production.\nThroughout the trial AEs managed give/gets, ensuring each RFP or additional codev session was tied to information about the prospect’s buying process.\nLooker was licensed as a SaaS offering where pricing was platform and user based. The pricing was typically custom based on the customer.\nGoogle Cloud Lessons\nGoogle Cloud was an entirely different beast. I was a data analytics specialist, an overlay to the GCP account teams who sold the entire GCP portfolio. (Actually I was a Looker specialist so I was an overlay to the DA overlay). If Looker was about value selling, GCP was about solution selling which meant:\nAccount team would meet to prepare a potential solution pitch for a customer, often something as broad as “better marketing analytics”.\nWe’d meet with the customer, usually with pretty high level stakeholders, and present the pitch. Often we led with the Google brand, the benefits of GCP, and then the solution. Occasionally we would do a brief demo.\nAfterwards, we’d iterate on further pitches and demos usually with lower level stakeholders.\nNormally POCs or even custom demos would be done by a partner funded by GCP.\nThis process was sometimes done with greenfield accounts but was often done within existing GCP accounts. There were two goals: to secure a commit and then to turn that commit into revenue by driving workload consumption.\nIt was not uncommon to have 10-20 people involved in a GCP opportunity in one form or another, and as an overlay overlay I usually spent 2-3 weeks with these teams before moving on.\nGCP is massive. Their systems are massive. But I did enjoy a few tools:\nGoogle runs its own version of Google on all of their internal sites, docs, etc and it makes it really easy to find information - and also really easy to go down rabbit holes investigating cool things totally removed from your work, like borg.\nGoogle has a special internal short link system that makes it easy to give memorable links to content. These go/ links are used extenisvely.\nOur office had a breakfast and lunch each day (Indian food and bacon were my highlights), a bike shop, a climbing wall, pinball, a barista, and fully stocked kitchens with snacks. Most people didn’t come in daily, and after the January re-org my “teams” were all based throughout CA.\nIn addition to the sales process I got exposed to GCP’s take on IAM, cloud consoles, and their crown jewel: BigQuery.\nI’d like to Thank…\nIn no particular order\nKate, who was my Looker mentor and taught me to give even better demos\nAlex, who goes to the office everyday, and is always available to hash out a problem\nHutch, who was my first Looker hiring manager and had the best manager quality: absorbing not amplifying anxiety\nAlyx, who gives the best executive demos\nScott, who was my manager in GCP and is authentic, genuine, and optimistic\nLuka, who put up with my questions about react\nQuynh, a CE onboarding buddy and sounding board for GCP madness\nDave, Rob, Tom B, AEs who taught me the ropes of Looker’s deal pursuit\nElementl\nSo what’s next? I’ll be joining Elementl as their first SE. I am excited to be back at a small startup where the sales strategy, pitch decks, and demos still need to be built. I’m also really excited about the technology. At Google I enjoyed getting more hands on with SQL running Looker plays. GCP sales, on the other hand, covers so many tools it’s impossible to have more than a shallow understanding of each. I’m looking forward to being back in the trenches selling, but also writing Python and wrangling Kubernetes. Elementl is a good fit for me because it is a technical sale to technical personas, with a small sharp team, built around a bottom’s up sales process.\nI am grateful to have “double-dipped” and learned from both Looker and GCP. I am also grateful for a slower year with time off to start learning how to be a father. While it is hard to leave the security and comfort and food of Google, for now I am looking forward to going back to startup “time”. Maybe I’ll return to the behemoth one day, but till then, thanks fellow Googlers!\n\n\n\n",
    "preview": "posts/2022-10-07-from-the-behemoth/images/bike.jpeg",
    "last_modified": "2024-04-23T20:22:42-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-23-dagster-part-1-dev-gcp-setup/",
    "title": "Dagster Getting Started",
    "description": "Steps to get started with dagster using GCP for \"local\" dev and Dagster Cloud + GKE for production.",
    "author": [
      {
        "name": "Sean Lopp",
        "url": {}
      }
    ],
    "date": "2022-09-23",
    "categories": [],
    "contents": "\nIntro\nThis post is, continuing my trend lately, incomplete. I recently learned a new tool, called Dagster, for orchestrating data pipelines. To learn this tool I followed a process:\nI did a cursory read through the documentation.\nI installed a sample project. This often requires a clean development environment.\nI played with the sample project, changing lines of code to see what would break, and attempting to test my understanding of the core concepts by making minor tweaks.\nThe first part of this post covers these three steps. After feeling like I understood the open source coding behind dagster, I wanted to follow a similar process for their SaaS offering, Dagster Cloud.\nI did a cursory read through the documentation. Dagster Cloud has a few “choose your own adventure” moments, and I picked a single path that I felt would introduce me to many of the core components: Dagster Cloud Hybrid + Kubernetes. This path was not the simplest - that would be Dagster Cloud Serverless - but I wanted to learn as much of the architecture concepts as possible.\nI attempted to follow the documentation for my selected path, using my modified sample project.\nThese two steps make up the back half of this post. After finding success with the modified sample project, I then try to re-implement something from scratch that I’ve built before. Usually my snow report. In my re-implementation I try to start simple and eventually use a large surface area of the new tool. This post ends with a brief summary of that process.\nI mentioned the post was “incomplete” - I’ve outlined the post but haven’t filled in all the commentary. :shrug:\nDevelopment Setup\nLots of people use their laptop for dev work. I prefer to use a small cloud VM for a few reasons:\nMy internet sucks. It can usually maintain a ssh connection, but installing new software is much faster onto a cloud VM.\nThe tooling for “local” remote work is really great in VS code through their remote host connection. All you need is ssh.\nI can use my dev environment from many computers, which often comes in handy.\nIn this case I:\nFollowed the GCP console to create a default compute engine VM with a public network\nConnected VS Code to the GCP VM using gcloud compute config-ssh\nDagster 101\nI followed the excellent dagster docs to bootstrap the library and the sample project:\nmkdir dagsterbootstrap\nmkdir dagsterbootstrap/env\nvirtualenv dagsterbootstrap/env\nsource dagsterbootstrap/env/bin/activate\nprintf 'dagster\\ndagit' >> dagsterbootstrap/requirements.txt\npip install -r dagsterbootstrap/requirements.txt\ndagster project from-example --name myproj --example assets_dbt_python\nAt this point I did my code reading and modifications. The result is in the loppster repo.\nThe changes I made:\nmake ML “model” persist in DB table\nmake ML “model” log starting values and optimized values for linear reg\nmake ML “model” starting values configurable\nPrep for Dagster Cloud 1\nAfter understanding my modified sample project I got ready to follow the steps for Dagster Cloud + Kubernetes:\nSign into Dagster Cloud w/ “Hybrid” option\nIn GCP follow default steps to create K8s cluster with “autopilot”\nCopy agent secret from Dagster Cloud\nIn cloud console (easiest place), run this command that ensures helm and kubectl use the cluster we just built:\ngcloud container clusters get-credentials autopilot-cluster-1 \\\n    --region us-central1 \\\n    --project myhybrid-200215\nFollow the dagster docs to create the namespace, setup the secret, and install the agent into the namespace with helm, with helpful commands being:\nkubectl config current-context\nkubectl get pods --namespace=dagster\nPrep for Dagstr Cloud 2\nThis is the part where we take our working dagster code and get it ready for deployment.\nIn the dagster example project, initialize a git repo, commit all the things, push to a GitHub repo\nAdd a Dockerfile with this content to the top-level directory of the example project (I copied this from their docs):\nFROM python:3.8-slim\n\nCOPY requirements.txt /requirements.txt\nRUN pip install -r /requirements.txt\n\nWORKDIR /opt/dagster/app\n\nCOPY . /opt/dagster/app\nAdd a requirements.txt file to the top level example project directory with the necessary packages, the list is in setup.py. You could also modify the Dockerfile to install your dagster project as a package and would pull in the deps through setup.py instead of requirements.txt\n(One time), create a place to store docker images\ngcloud artifacts repositories create dagit --repository-format=docker --location=us-central1 --description=\"DAGS\"\n(One time, before we forget), go to the GCP IAM console and select the box for “Include Google-provided role grants”, for the Compute Engine Service Agent and Kubernetes Engine Service Agent, select the box, select the edit pencil, and add the role “Artifact Registry Reader”\nHave GCP build your code into a docker image and push to the registry\ngcloud builds submit --region=us-central1 --tag us-central1-docker.pkg.dev/myhybrid-200215/dagit/loppster\n(Prob one time), setup the code location in Dagster Cloud using the result from above. Future updates can be done by deploying a new image and clicking “redeploy” on the code location page.\nlocation_name: prod\nimage: us-central1-docker.pkg.dev/myhybrid-200215/dagit/loppster\ncode_source:\n  package_name: assets_dbt_python\nBuild a New Project\nNow that I felt comfortable with dagster and Dagster Cloud I decided to re-implement my snow report project. The extensive details are documented in the repository ReadMe. My general approach was:\nBuild the scaffolding for the project\nStart out locally, and take the functions I had for pulling data from an API and cleaning it and place them in the dagster paradigm\nWork through the dagster concept of resources, and switch from just local resources to using BQ and GCS\nAttempt to deploy my basic implementation (this time using GitHub actions instead of the manual approach outlined in this post)\nFight GCP IAM and GitHub Actions. Subdue them\nAdd tests\nRefactor my code, asking questions in the dagster Slack\nAdopt Dagster Cloud’s GitHub action for branch deployments following their guides\nAdd more complexity to the project by bringing in partitions\nAdd more complexity to the project by brining in dbt\nThis sequence is clearer in hindsight, and was developed step by step. There was also plenty of trial and error - see the GitHub commits and Action runs for the comical set of typos, mistakes, and mis-understandings.\nI really appreciate the VS Code development tools, the Python debugger, the Python notebook debugger, and of course dagster. It is a great time to be alive and writing software!\n\n\n\n",
    "preview": {},
    "last_modified": "2024-04-23T20:22:42-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-16-modern-data-stacks/",
    "title": "Modern Data Stack_s_?",
    "description": "An argument and map for 4 modern data stacks.",
    "author": [
      {
        "name": "Sean Lopp",
        "url": {}
      }
    ],
    "date": "2022-02-16",
    "categories": [],
    "contents": "\nUnfinished postI decided this post was a bit too… uh.. pandering I guess? I want posts that future me will find useful, and that usually means they need to demonstrate an implementation. This post is more “all talk, no walk”. While landscapes can be useful, I didn’t see further ROI in flushing out this outline. That said… I made a picture… I’ll unscrupulously put it online.\nI get unreasonably excited about maps, especially maps of technology landscapes. The challenge with maps is always the granularity. How far do you zoom in? Too broad a map is useless, but too narrow a map might leave out key alternative routes. In the data space, there are many great maps around the evolving “modern data stack”. Unfortunately I think many of those maps are too zoomed in. For the past few months I’ve struggled to rectify discussions about the modern data stack with an onslaught of VC fundraising and valuations1. Where does the billion dollar Databricks fit into the dbt narrative about analytic engineers? How does Snowflake continue to beat expectations despite having a single product - not a stack?\nI believe there are actually 4 modern data stacks. In this post I’ll unpack a map of these 4 modern data stacks, I’ll defend a few key omissions, and I’ll also explain why I’m especially bullish on Google’s position in this landscape.\nDisclaimer: I work for Google Cloud. While I’ve certainly drunk some Koolaide, many of the arguments I make in this post are reasons I joined Google not just evangelical marketing I’ve internalized. This post represents my own views and not that of my employer - everything here is speculative!\nThank yous: Many existing maps inspire mine, especially those drawn by Tristan Hardy, Benn Stancil, Bruno Aziza, and Priyanka Vergadi.\n\n\n\n\nThis map includes 4 “data stacks” represented by the four colored paths.\nReal results with the fewest tools\nThe green path represents a data stack found in many digital startups, or in organizations without a formal data team. Data sources, especially website events, ad events, and purchase data are analyzed. This analysis often starts where the data lives, e.g. using Google analytics reporting for website events. To answer meaningful questions like cost-of-acquistion teams pull data out of those tools and join them. While these joins can occur in Spreadsheets, I believe there is immense benefit introducing a data warehouse. Within Google Cloud many data sources can be directly imported to BigQuery and immediately analyzed in SQL or used to power warehouse-connected spreadsheets.\nIf you are asking yourself if this is really a data stack or just a recipe for disaster - you’re not alone. Proponents of the “modern data stack” argue that organizations taking on a data warehouse should also invest in proper data connectors, version controlled transformations, and a robust BI tool with a metrics layer.\nI agree these additions are all beneficial, but I also think it is important to recognize crawling as a distinct and meaningful milestone! Imagine being the sole data person at a start-up or non-profit. You probably know quite a bit about your data, but you may not be in a position to navigate the procurement process for data connectors, the value-add of Git, or the steps to administer a BI tool. I’m excited that with BigQuery anyone who can upload a file or navigate Google Drive is only a step away from a warehouse.\nThis green path is also where I would put Tableau, Alteryx, and PowerBI. However, I do not count these tools among a “modern data stack” because they encourage pulling data out of the warehouse and then joining and centralizing the data within the tool. A key criteria of a modern data stack is that it should encourage loading and transforming within the warehouse.\nWalking\nMost people referencing the modern data stack think of the yellow path. Like walking, as a form of human transport, this data stack is the mode of data stacks. However, I think it would be a mistake for a data engineer or data executive to be content with only ever walking. For many large organizations, this definition of the modern data stack may be of little help. Should an organization with an on-prem Cloudera deployment be focused on Fivetran and dbt?\nRunning, ergh, I mean migrating\nThe red path provides a meaningful alternative to organizations who are hoping to modernize a traditional data stack. These organizations, like large banks or retail giants, have made big investments in data systems, transformation tools, and infrastructure using tools like Oracle, SAP, Teradata, and Informatica.\nMaybe one day it will be possible to replace these systems with off the shelf data connectors, but today these modernization efforts require care. Organizations can still benefit from modern server-less systems, but those systems will use tools like federated queries on BigTable and Cloud Spanner, highly custom loading in DataFlow, and Dataproc transforms (the serverless managed equivalents of Apache Hive, Beam, and Spark). This space is also the motivator behind tools that query data without a central warehouse, data lakes and query engines like Presto, Athena, and Dremio.\nRunning, ergh, I mean streaming\nThe blue line\nI believe both “running” scenarios merit discussion when an organization thinks about the evolution of their data stack, and also deserve thought from consumers of the modern data stack such as reverse ETL tools, machine learning pipelines, or purpose-built data applications.\nMissing Players\nDatabricks\nSnowflake\nR & Observable?\nWhy Google?\nGreen path with 1st party data\nAuth that “just works” in the Google ecosystem\nBigQuery (ML / GIS / serverless)\nReally hard engineering solved for the purples (BigTable, Cloud Spanner, DataFlow)\nConclusions\nI’m excited to see how this market continues to evolve and only a little fearful that it might pop. Tweet me if you have a map.\n\nIt may be a fool’s errand to try and rationalize anything to do with market valuations. If it turns out we’re in a mega-bubble than rest assured I’ll be the first to throw out all of the maps.↩︎\n",
    "preview": "posts/2022-02-16-modern-data-stacks/images/ELTA_high_res.jpg",
    "last_modified": "2024-04-23T20:22:42-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-08-to-the-behemoth/",
    "title": "To the Behemoth",
    "description": "On leaving RStudio to join Google",
    "author": [
      {
        "name": "Sean Lopp",
        "url": {}
      }
    ],
    "date": "2021-09-08",
    "categories": [],
    "contents": "\nIn November 2015 I scored my dream job at RStudio, the company behind the RStudio IDE, shiny, and the soon-to-be-named tidyverse of R packages. Looking back, that offer was definitively my lucky break. I joined as the 3rd member of their sales team, the 22nd person in the company as a whole. I knew nothing about startups, I definitely had no business being in sales, but I was estatic when I shared with my grad school buddies that my new email would be sean@rstudio.com.\nFast forward just shy of 6 years. RStudio has grown to 200 employees, our sales engineering team is approaching 20, and I am sad but excited to be moving on. This post is really written for 2028 Sean, to remember what I thought and felt at this moment. To remember the people who have been so important, and to document my speculations on the market and the world so that in 6 years I can laugh at all the unknown unknowns.\nThe somewhat short story of my time at RStudio\nIt is hard to summarize six years, especially when those 6 start-up years map to at least 12 years of regular company work. My attempt:\nFrom 2015-2016 I worked as a customer success rep. I learned what commission was, what a non-recoverable draw entailed, and all about pharmas (our largest vertical), salesforce, and marketo. It was hard to believe that RStudio would trust a 22 year old to jump into calls with some of the largest companies in the world, its even harder to believe it worked. I still remember when my boss, the wise and sales hardened Jim Clemens, took me to a fancy bar my first day onsite in Boston. I don’t think he or the bartender believed I was 22 and had an expense account! Other critical folks in this time included Pete Knast (who introduced me to sales and real expense account travel) and Phil, who would become one of my closest partners in crime and the future director of life sciences.\nI remember a few things about starting at RStudio that really tell how green I was. At that time I was living in a horrible housing complex where I had my own room but shared a shower and kitchen with 8 strangers. I paid 400 a month but didn’t have internet. I took my RStudio interviews from the floor of a college laundromat lobby. My first day was a Monday in RStudio’s Boston office. I had booked a red-eye flight that departed Denver at 12am to avoid having to pay for an extra night’s hotel fair. Unfortunately Saturday I realized that I had messed up the booking and my flight left denver Monday at midnight. I paid a $600 last minute change fee that I never expensed out of embarrassment, but set me back a months rent. When I first met RStudio’s president, Tareef, I called him Kareef. He politely corrected me and then asked me how Colorado was. I said fine, and then he asked if I had ever been to Boulder. I jokingly said “the place where they believe in all that gluten nonsense?”. I later learned Tareef was an ardent gluten-free adherent. It wouldn’t be the last time I put my foot in my mouth, but I am pretty sure after the first 5 minutes Jim thought he had made a horrible mistake.\nIn Oct 2016 I jumped from RStudio’s sales org to become the second solutions engineer. At the time this move was pretty straightforward. Nathan Stephens, the head of SE, was a generous man (who would turn out to be a dear friend and mentor). I had the technical skills to support our growing sales team, and I liked doing demos more than salesforce. Our team grew quickly over the next year, key hires included Jonathan R (the finance guru who would first break my naivety bubble and tell me about the business world outside RStudio) and Edgar Ruiz (another solutions engineer with unquenchable cheer and serious Latin coolness).\nA brief short story to reassure you that I still hadn’t mastered this professional life. One week in 2016 I was asked to fly to San Jose to be the sales representative in a series of customer visits that the Hadley Wickham had agreed to attend. I would later learn that Hadley was a literal rock star, with weirdo R users asking for his autograph. My single job during this tour was to drive Hadley around. We had a full day, with a plan to meet at the airport, then drive to Intel, Genetech, and into San Fran to visit Uber. (Uber had such a weird culture, and later would be the only customer I would legitimately complain was rude and arrogant.) I showed up at the San Jose airport at 7am and texted Hadley. We were going to meet at 7:30 and arrive at Intel in time for an 8am meeting. By 7:40 we were both confused, he was texting me where he was, but I couldn’t find him. Turns out that, “the airport”, was SFO, not SJO. I had failed at my one job, and Hadley had to scramble to pay a Lyft 100 dollars to drive him from SFO to Intel where he calmly gave his hour long presentation in 20 minutes.\n2017-2018 is what I’d consider my golden years. We hired Cole Arendt - the single smartest person I’ve ever worked with, despite his tendency to hack faster than he could ever document. I traveled all over, including Austin, Boston, Seattle, New York, San Fran, Chicago, and Houston (where I got an exclusive tour of NASA mission control before giving one of the worst demos of my life because, while they could send a man to the moon, NASA couldn’t provide reliable public internet.). Everywhere I went we got to eat amazing food, and I filled any spare time walking the streets or visiting art museums. Overtime I would also travel to Belgium (my first trip abroad) and London. I rediscovered biking (thanks to the prodding of Aron Atkins to buy a Salsa Warbird). I gave my single best presentation at the 2018 RStudio Conf in Orlando showing in real time how to scale a shiny app to 10,000 users. (The Orlando conf was incredible. We started with a RStudio exclusive evening at Harry Potter World and then JJ paid for the RStudio employees to spend the next day at the park, fast passes included! This also happened to be 7 days after my wedding…. quite the “honeymoon”?).\nTowards the end of 2018 I started to dabble my toes in product management. I joined the sprint reviews for RStudio Connect and I started working closer with Jeff Allen (an enormously kind and talented engineering manager) and Robby Shaver (a brilliant but laid back designer who taught me that less is more in words, buttons, and just about everything but white space). (Side, side note: for some reason I consistently lied to Jeff about my age, claiming I was a few years older than I really am. This lie started because I was embarrassed to take on responsibilities while being so young, but because Jeff is so thoughtful it ultimately became worse and worse - one night he politely realized I was lying when my time in college, high school, and at RStudio didn’t add up. Sorry Jeff!). I became more and more involved in our product planning, and eventually I was given the reigns to create a new product from scratch in RStudio Package Manager. The genesis of this product was partly a need to support customers in offline environments, but mostly my ambition to lead a product team. We started with a gaggle of spreadsheets and customer interviews, and thanks to Jon Yoder (a humble, quiet, and brilliant engineer) we built something special. Package Manager would fully launch in 2019, and I would lead the team through 2020. The product eventually became both a core part of RStudio’s offering, accounting for millions in revenue, and it became a public service. I am most proud of how Package Manager provides pre-compiled packages for Linux. This work took a team and some clever planning, but in it I feel I’ve left a lasting fingerprint on the R community. Before, installing R and packages on Linux could easily take hours. Now, both R and packages come pre-compiled from RStudio. The way I know I’m successful is that occasionally the service will fail and folks will have to go back to the old method, and it is so painful as to be unfathomable! Of course none of this success was pre-destined, I learned that building a product requires countless trade-offs and a near constant diligence in engineering groomings and reviews. (Even great engineers have a tendency to never ship if left to their own choices.) During this period I also got to work on marketing content with Greg Swinehart, who is perhaps the best human being I’ve ever met.\n2019 and 2020 marked the beginning of the end, though I didn’t know it at the time. In mid-2019 I formally became a full time product manager – RStudio’s first. I was woefully unprepared, and in hindsight, this position wasn’t really a fair one. I was tasked with balancing our executive team’s ever-changing vision, our legitimate but diverse customer requests, and our dev team’s daily execution. I was soon crushed. Luckily at this time I was able to rely on the wisdom of Kris Overholt and Daniel Rodriguez, two ex-PMs who briefly worked at RStudio and instilled in me both an earnest skepticism of the business and a keen nose for bullshit. At this time Jeff left to pursue a new team at RStudio, and so I found myself managing both RStudio Package Manager and RStudio Connect. A wiser person would have built a product managemnt team, I attempted to forge ahead on my own. I also refused to give up my work with customers which I rightly thought would ground my product work, but which I incorrectly assumed would magically take less time. A perfect recipe for burnout. This was also the period when I began to realize an unhealthy relationship with Tareef, RStudio’s president. I hesitate to write much about Tareef, I won’t need a blogpost to remember him. He was both a paternal figure genuinely invested in my career, an interesting corporate philosopher, and my strategic arch-enemy. I wanted to build a process that took data from our customers, aligned around a strategic vision, and then executed against a prioritized roadmap. In many ways Tareef wanted the same thing, but lived at a different level of visibility. He struggled to share his vision, and I lacked the leadership tools to clarify it, and so I often felt undermined when he would critique the team’s execution or focus while simultaneously introducing new initiatives. The only way I lasted until the end of 2020 was because I was joined in product by Kelly. Kelly is one of the most strategic and empathetic people I’ve ever met, and in Jan 2020 she joined me in working on Connect. Mentoring Kelly was hard for me, not because of Kelly - whose work was always brilliant and 3 steps ahead of my own, but because the challenges she felt and reported back to me caused me 10x the frustration as when I felt them myself. I realized during this time just how hard it can be to be a manager or mentor, I was much better suited to dealing with shit than telling someone I cared about that I couldn’t remove their obstacles. By Oct 2020 I was constantly angry, burnt out, and quick to boil into frustration with others. My wife said I had to quit. I had promised Tareef if it came to that I would give him a few months notice, so in October I told him I needed to leave. A few other things are worth noting about the fall of 2020. For one, my old solutions engineering team was imploding. Skeptical Kris had left, leaving a wake and a big hole (because he was also the most gifted solutions engineer I’ve worked with). Cole and James (who was and is my constant motivation in RStudio’s cycling strava) were both on paternity leave. Edgar, the other old goat from 2017, had abruptly quit at the beginning of the year. This proved a tough load, and it ultimately contributed to Nathan stepping away from the director role. In short, it was a total and complete dumpster fire, and I found myself in the midst of my struggles with product attempting to hold together the fragile remains of the SE team.\nEnter 2021. My big question after telling Tareef I was done was “what’s next?”. This question turned out to be quite the doozy. I temporarily opted to return to solutions engineering, and like most things in my life this short term choice turned out to be insanely lucky. After 9 months of reflection it is easier for me to understand that sales engineering is where I should continue next, but that realization came after quite a few false starts. I applied and almost accepted a consulting job that would have returned me to my college work of energy analytics and 10 hour days of coding. I applied and was accepted to law school. I even read all about other product roles, teams, and domains. In June I took two weeks off to attend a wedding in Alaska and travel the state. During this time, and there was a LOT of time driving, I realized that I really enjoyed my time with our sales team. I have been lucky to work alongside really excellent account executives in Rachael Dempsey (also the most gifted community marketer I’ve ever met), Jason Miles, Kevin Hayden, Sabrina, Lauren, Nichole, Dylan, and countless others. Unlike product management, where the objectives are ambiguous and the time frames are measured in years, sales had a clear directive. Yes, there is still an art to asking questions and aligning around value, but our team was always on the same page. The rewards were satisfying and aligned with compensation. The customers were diverse and interesting. Finally, in my year of renewed SE focus, I rediscovered that I was really good at sales engineering. I had unknowingly felt like an imposter as a product manager, always quite certain that I had no business representing the needs of any users, desperately trying to read the market tea leaves while directing millions of dollars in dev investment. Ultimately I realized I was in the right role as an SE, which led to a natural question, how do I get better and go further? This question has taken awhile to answer, but it is what leads me to Google. I need to be exposed to something outside of RStudio, to learn how other organizations and sales teams operate, to sell different products, in different frameworks, to different customers. I realized, with both happiness and sorrow, that I need to leave RStudio.\nYou may be wondering whats to come of the SE team that was struggling in 2020 and that I luckily rejoined in earnest in 2021. To answer that question I have to introduce a few more characters. Alex Gold joined RStudio in 2019 as an SE after giving an excellent interview with a few too many memes. He is proving to be both a talented “get shit done” organizer and a thoughtful manager who will carry the team. Andrie joined RStudio in 2017 and has had a knack for doing the dirtiest work, and I expect he will continue that trend. David joined RStudio in 2020 and shares my knack for sarcasm, dread, and picking up new tools quickly and holistically. Others deserve mention as marvelous people who I worked with less but still hold dear: Gagan, Ian, Ralf, Mark, and Lou (the most diligently organized product marketer).\nIt is also worth mentioning that during 2021 I had the privilege of working with an executive coach. I spent 6 months working every other week with an older gentlemen named Harry. Harry’s superpower is very quickly seeing to the heart of things. The main thing Harry helped me realize was that I was responsible when people misunderstood me, or I misunderstood them. He taught me to convince others by teaching them not telling them (slow down and ask questions you sometimes know the answer to). He taught me to ask questions I didn’t know the answer to, again and again, to try and help others show me their work. Most of all, he taught me that Pixar’s Inside Out is actually pretty accurate, and that feelings serve their best purpose when they work together. Or, the Aristotelian take, prudence is the heart of all virtues.\nIn closing this short story, I still struggle to imagine that this chapter is coming to a close. I can still put myself in the middle of some of my best and fondest memories:\ndrinking a beer with Phil at O’hare after a long drive from Abbott\nsitting in between Alex, Greg, and Joe C at a bar talking about music\nyelling at a planetarium screen after one too many drinks with Tyler while Kelly watched mortified, and then later that evening finding my way into an amazing halal place with Alex\nsitting with Nathan at a poolside lounge talking for hours about the importance of good mattress selection\nsipping wine with Mark E in Chicago, pretending I could tell the difference between four reds\neating an absurdly expensive hamburger with Jonathan off of Wall Street\ndriving through a blizzard to spend 3 nights onboarding Jason at Jim’s lakeside mansion/cabin, and our “team building” 2 mile hike post-holing through 2 ft deep snow up the side of a mountain. (Jim is in remarkable shape)\nwalking around Mountain View at 11pm eating ice cream with Phil and Tareef after meeting with a bunch of pharmas, listening to Tareef tell us his life story\nattending JJ’s birthday party, complete with amazing finger food and dancers spray painted to look like robots\nriding the London tube for the first time, and then 8 days later for the last time, feeling rather comfortable as I recognized the route names\nnavigating the middle of no-where Cambridge with Tareef trying to find AztraZeneca’s headquarters, and Tareef’s kindness when he stopped at a small cafe even though he doesn’t drink coffee\neating benets in New Orleans late at night with our whole SE team\narguing about Looper in a hipster food court\nA shorter TL:DR of a few things I learned at RStudio\nInbound sales can be amazing. RStudio built a business with almost zero marketing, close to 3 million a rep, and an average deal size of 30K ACV. While the deals got larger and the account load smaller, I learned that an open source community, great products, and low pricing can sustain an incredibly leveraged business.\nOn-premise products have pros and cons. Integrating with disparate architectures is hard, and on-prem system admins can be hit or miss. BUT, for free, we were able to position ourselves as multi-cloud or hybrid, and our marginal costs were truly $0!\nStock options. They are a lottery, but worth exercising early if they are cheap.\nAs companies grow its easy to think the sky is falling with every change. Hold back chicken little.\nThe sales team really does eat and drink the best. Hang out with them when you travel.\nMy most lasting product efforts required the alignment and patience of a large team. My most fun efforts required under-the-radar teamwork with 1 or 2 individuals who knew how to get shit done.\nInterviewing is really hard. Be patient and hold out for people who generate an absolute yes. Any qualities you notice in an interview (sloppy with details, talkative, a little arrogant) will become more and more pronounced when you work with them. People who know their shit can talk about it at different levels, and will lead you deeper. (If YOU have to keep asking questions to probe for someone’s concrete knowledge, its likely they don’t have much!) Further, if someone explains something and you don’t understand it, its not because you are a dummy – they’ve failed the task.\nMarketing is important, but easy to shit on because no one can really say if its working. When a deal is closed won its “because of sales”, when a deal is closed lost, its “because of product”.\nSmall teams of independent professionals are amazing, but they are really hard to scale. Larger teams need more process to stay afloat.\nAs a leader, “showing up” to every meeting is important. No team meeting or 1:1 can be handled casually, so skip it if you’re not in the right head space. Your job is almost always to present ideas with optimism, absorb (not amplify) the natural concerns around ambiguity, and remain professional. All that said, it can be really helpful to have a few peers to whom you can vent and “let down your guard”, but gossip isn’t a shortcut to being considered genuine.\nKeyboard shortcuts in gmail are clutch, as is Alfred, zsh, the reveal bullshit plugin, and hard restarting chrome every morning.\nAfter countless hours in many airports I can confirm that O’hare is the worst, JFK is the most inconvenient, and as Phil would proudly tell you, Indy is the best. A few other airport comments - if you are going to downtown Chicago, take the blue line not a rental car. (Except if it is Lalapalooza week, then definitely do NOT take the train.) In Denver, the fastest way to get from driving to your gate is to park in the southern end of the west lot, walk through the Westin, up the escalators, through South security, and onto the train. Patagonia black hole 40L duffles are the optimal 2-3 day trip bag as they can fit under the seat in a pinch and fit on your back. People who roll their luggage are slower, can’t run down escalators, and look like idiots walking downtown. Never gate check a bag. Only fly direct. TSA pre-check is the single best investment you can make.\nThoughts about Data, Analytics, and Statistics\nAnyone who knows me well (or even a little) knows I hate the Gartner Magic Quadrant. Here are my anecdotal hot takes.\nJavaScript might eat the world, but SQL already has and will again. I am very bullish on groups like Observable. Data products that only need a browser are pretty sweet. Those products, however, aren’t great at data prep – enter SQL. I was slow to realize that every meaningful bit of code I’ve dogfooded at RStudio has just been SQL in disguise. I’m super bullish on products that make SQL better, R included? I can’t wait to follow Dremio, Voltron Data, and dbt. Those are 3 rocket ships.\nI love RStudio Connect. Domino and Dataiku I really don’t understand. If you want infrastructure and project orchestration then Sagemaker, GCP Notebooks, Azure ML, etc are sufficient. Connect is unique in it’s ability to host an R or Python app outside of a development context. While Connect hasn’t quite figured out if its a BI tool for data scientists or an execution service for R and Python (like Heroku), I sort of love that it lives in its own grey area. I am certain Kelly will ensure it keeps delighting users despite its lack of a coherent category.\nSome modelling activities are actually data engineering in disguise. NLP, image recognition, deep learning, most of them are just to get non-tabular data into a SQL-able form. Corollary 1: Most ML hyped production use cases are either just counting or could be served by just counting. Corollary 2: Databricks is valuable for teams that need to do SQL on larger-than-SQL data, but SQL warehouses are getting bigger and bigger so I really don’t understand their multi-billon dollar valuation. The rest of their lineup, MLOps and what not, probably isn’t very real.\nStatistics is all about helping identify if an insight discovered in an analysis is legitimate or could be the result of random chance. In many cases, executive leadership won’t care. Boxplots are better than bar charts, but many executives won’t get them.\nThe FDA is the most insane regulatory body I have ever encountered, and paints a grim picture of what “digital transformation” looks like in government. Feel what you will about big pharma, the idea that they have to submit data on clinical trials as plain text files according to a 1999 spec is crazy. SAS rules this land because the FDA is unwilling to change, and their claim to be language agnostic is tantamount to fraud.\nMost data science companies, especially start-ups, are consultants in disguise. This has pros and cons. Consulting keeps you close to your users and helps build better products. It can also lead to VCs pouring stupid amounts of money into vaporware (ahem - plotly, streamlit, and also IBM Watson?).\nh2o.ai is legitimate and building interesting AI techniques. Domo and DataRobot are not.\nKubernetes lives up to the hype and its worth learning. Overtime the patterns make sense and the helm templates become readable. Chef recipes, on the other hand, will never make sense.\nLooking towards Google\nI’m not sure what to expect next, except that Google will be different. I hope this pandemic ends and I get to enjoy their famed lunches. I hope that I can get up to speed on BI, that SQL really does continue to eat the world, and that JavaScript becomes less confusing. I am excited to learn about the GCP landscape, especially data engineering which is a critical part of my SQL + JavaScript dominates the world prediction. Most of all, I hope I am fortunate enough to find myself with a team that is half as humble, hungry, and kind as my current colleagues and friends.\n\n\n\n",
    "preview": {},
    "last_modified": "2024-04-23T20:34:12-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-18-boxplots-and-p-values/",
    "title": "Boxplots and p-values",
    "description": "Do boxplots provide good intuition about hypothesis tests?",
    "author": [
      {
        "name": "Sean Lopp",
        "url": {}
      }
    ],
    "date": "2021-06-18",
    "categories": [],
    "contents": "\nA long time ago, perhaps in my college’s math deparment computer lab, I recall someone mentioning that boxplots could give you an intuition for t-test results. The intuitive rule was:\n\nIf a set of boxplots overlaps then there is no statistical difference between the two samples. If the boxplots do not overlap then perhaps there is a statistically significant difference.\n\nFor whatever reason, probably because I am a visual learner, this intuitive rule has stuck. Setting aside the obvious problems around formulating a hypothesis and threshold, I became curious as to whether this intuitive rule was correct, semi-correct, helpful, or flat out wrong.\nAm I a sucker if I use boxplots as a statistical shortcut for comparing groups? Does the intuition here present any “gotchas”? Any scenarios where the intuition and the actual result of a hypothesis test are vastly opposed? Why do no stats articles online that introduce tests use boxplots to create intuition??\nTo answer some of these questions I decided to run some simulations. (Ok actually I decided to Google, but surprisingly I didn’t find any great articles on this topic?!) I also briefly contemplated if this boxplot/p-value relationship could be derived mathematically… but to Professor Navidi’s dismay I gave up pursuing that path before writing a single equation down.1 I love simulations because they are easy. No messy data. You control the “real” answer. Such a relaxing contrast to real data science or, even messier, engineering management!\nIn this post I present a few results along with my hot-takes. I am hoping the twitter stats hive mind can weigh in and set me straight. Come on you PhDs, set this grad school dropout straight!\nA final, unrelated question - does anyone know how to create a boxplot on large data? Is there an efficient boxplot “map-reduce” alogrithm? Update: this SO thread basically has the answer!\n\n\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(tidyverse)\nlibrary(ggpubr)\nset.seed(8)\ntheme_set(theme_minimal())\n\n\n\n\n\npopulation_plot <- function(x,y){\n  tibble(\n    data = x,\n    dist = \"x\"\n  ) %>% \n  rbind(\n    tibble(\n      data = y,\n      dist = \"y\"\n    )\n  ) %>% \n  ggplot(.) + \n    geom_density(aes(x = data, fill = dist), alpha = 0.3)\n}\n\nadd_samples <- function(pplot, x_samples, y_samples) {\n  samples_df <- tibble(\n    data = x_samples,\n    y_coord = 0.05,\n    dist = \"x\"\n  ) %>% \n  rbind(\n    tibble(\n      data = y_samples,\n      dist = \"y\", \n      y_coord = 0.1\n    )\n  )\n  pplot +\n    geom_jitter(data = samples_df, aes(x = data, y = y_coord, color = dist)) + \n    labs(\n      x = NULL,\n      y = NULL, \n      fill = \"Distribution\", \n      color = NULL\n    ) \n    \n}\n\nboxplot <- function(x_samples, y_samples) {\n  samples_df <- tibble(\n    data = x_samples,\n    y_coord = 0.05,\n    dist = \"x\"\n  ) %>% \n  rbind(\n    tibble(\n      data = y_samples,\n      dist = \"y\", \n      y_coord = 0.1\n    )\n  )\n  ggplot(samples_df) + \n    geom_boxplot(aes(x=data, fill = dist)) + \n    theme_minimal() + \n    labs(\n      x = NULL,\n      y = NULL, \n      fill = \"Distribution\"\n    )\n}\n\n\n\nNormal\nI decided to start by simulating normal distributions.\nTwo normal distributions, same mean, small variance\nMy first simulation presents a good example of “boxplot overlap”, where the alternative hypothesis that the samples come from different distributions would not be supported. In other words, high overlap, large p-value.\n\n\nx_mean <- 1 \ny_mean <- 1\nx_sd <- 1\ny_sd <- 1\nx <- rnorm(1000, mean = x_mean, sd = x_sd)\ny <- rnorm(1000, mean = y_mean, sd = y_sd)\n\nn <- 10\nx_samples <- sample(x, n)\ny_samples <- sample(y, n)\n\npop <- population_plot(x,y) %>% \n  add_samples(x_samples, y_samples)\nbox <- boxplot(x_samples, y_samples)\n\ntest <- t.test(x_samples, y_samples)\n\nlabel1 = sprintf(\"x mean: %d, sd: %d, y mean: %d sd: %d\", x_mean, x_sd, y_mean, y_sd)\nlabel2 = sprintf(\"Sample size: %d, p-value: %g\", n, test$p.value)\nggarrange(pop, box, labels = c(label1, label2), font.label = list(size = 8))\n\n\n\n\nTwo normal distributions, same mean, large variance\nMy second experiment presents a slightly harder “test” by increasing the variance. In this case the boxplots still overlapped, correctly suggesting the two samples come from distributions that are similar enough to reject an alternative hypothesis.\n\n\nx_mean <- 1 \ny_mean <- 1\nx_sd <- 10\ny_sd <- 10\nx <- rnorm(1000, mean = x_mean, sd = x_sd)\ny <- rnorm(1000, mean = y_mean, sd = y_sd)\n\nn <- 10\nx_samples <- sample(x, n)\ny_samples <- sample(y, n)\n\npop <- population_plot(x,y) %>% \n  add_samples(x_samples, y_samples)\nbox <- boxplot(x_samples, y_samples)\n\ntest <- t.test(x_samples, y_samples)\n\nlabel1 = sprintf(\"x mean: %d, sd: %d, y mean: %d sd: %d\", x_mean, x_sd, y_mean, y_sd)\nlabel2 = sprintf(\"Sample size: %d, p-value: %g\", n, test$p.value)\nggarrange(pop, box, labels = c(label1, label2), font.label = list(size = 8))\n\n\n\n\nTwo normal distributions, different mean, small variance\nMy third simulation gets a bit more exciting. Does the boxplot intuition hold up if the two samples actually come from different distributions? I decided to start with an easier case where the distributions had different means and small variance. The boxplot rule seems to get this one correct, though interestingly the p-value here would be above a 0.01 threshold.\n\n\nx_mean <- 1 \ny_mean <- 2\nx_sd <- 1\ny_sd <- 1\nx <- rnorm(1000, mean = x_mean, sd = x_sd)\ny <- rnorm(1000, mean = y_mean, sd = y_sd)\n\nn <- 10\nx_samples <- sample(x, n)\ny_samples <- sample(y, n)\n\npop <- population_plot(x,y) %>% \n  add_samples(x_samples, y_samples)\nbox <- boxplot(x_samples, y_samples)\n\ntest <- t.test(x_samples, y_samples)\n\nlabel1 = sprintf(\"x mean: %d, sd: %d, y mean: %d sd: %d\", x_mean, x_sd, y_mean, y_sd)\nlabel2 = sprintf(\"Sample size: %d, p-value: %g\", n, test$p.value)\nggarrange(pop, box, labels = c(label1, label2), font.label = list(size = 8))\n\n\n\n\nTwo normal distributions, different mean, small variance, larger sample size\nRepeating the same setup but increasing the sampe size shows the first failure in the intuitive rule.\nThis time the boxplots do overlap, but the p-value would meet a reasonable threshold and support the alternative (and correct) hypothesis that the samples come from different distributions!\nInterestingly, we could refine the intuitive rule a bit and satisfy this scenario. A refined rule might state:\n\nIf the median in one boxplot does not overlap with the IQR from another boxplot, the groups are likely statistically different.\n\n\n\nx_mean <- 1 \ny_mean <- 2\nx_sd <- 1\ny_sd <- 1\nx <- rnorm(1000, mean = x_mean, sd = x_sd)\ny <- rnorm(1000, mean = y_mean, sd = y_sd)\n\nn <- 100\nx_samples <- sample(x, n)\ny_samples <- sample(y, n)\n\npop <- population_plot(x,y) %>% \n  add_samples(x_samples, y_samples)\nbox <- boxplot(x_samples, y_samples)\n\ntest <- t.test(x_samples, y_samples)\n\nlabel1 = sprintf(\"x mean: %d, sd: %d, y mean: %d sd: %d\", x_mean, x_sd, y_mean, y_sd)\nlabel2 = sprintf(\"Sample size: %d, p-value: %g\", n, test$p.value)\nggarrange(pop, box, labels = c(label1, label2), font.label = list(size = 8))\n\n\n\n\nTwo normal distributions, different mean, large variance\nIn this scenario the original formulation of boxplot intuition aligns with the t-test. Both respect the large variance and the small sample size concluding that we probably don’t have enough information to support the radical hypothesis2. Interestingly our refined rule actually gets the “truth” correct - the two samples come from different distributions. But to quote Judas, “what is truth”?\n\n\nx_mean <- 1 \ny_mean <- 3\nx_sd <- 3\ny_sd <- 3\nx <- rnorm(1000, mean = x_mean, sd = x_sd)\ny <- rnorm(1000, mean = y_mean, sd = y_sd)\n\nn <- 10\nx_samples <- sample(x, n)\ny_samples <- sample(y, n)\n\npop <- population_plot(x,y) %>% \n  add_samples(x_samples, y_samples)\nbox <- boxplot(x_samples, y_samples)\n\ntest <- t.test(x_samples, y_samples)\n\nlabel1 = sprintf(\"x mean: %d, sd: %d, y mean: %d sd: %d\", x_mean, x_sd, y_mean, y_sd)\nlabel2 = sprintf(\"Sample size: %d, p-value: %g\", n, test$p.value)\nggarrange(pop, box, labels = c(label1, label2), font.label = list(size = 8))\n\n\n\n\nTwo normal distributions, different mean, large variance, larger n\nFinally, I repeat the test above but increase the sample size. In this case everything flip flops, the test correctly supports the alternative hypothesis that the distributions are different. Both formulations of the boxplot intuition fail. The boxplot intuition fails to increase its “confidence” based on the larger sample size.\n\n\nx_mean <- 1 \ny_mean <- 3\nx_sd <- 3\ny_sd <- 3\nx <- rnorm(1000, mean = x_mean, sd = x_sd)\ny <- rnorm(1000, mean = y_mean, sd = y_sd)\n\nn <- 100\nx_samples <- sample(x, n)\ny_samples <- sample(y, n)\n\npop <- population_plot(x,y) %>% \n  add_samples(x_samples, y_samples)\nbox <- boxplot(x_samples, y_samples)\n\ntest <- t.test(x_samples, y_samples)\n\nlabel1 = sprintf(\"x mean: %d, sd: %d, y mean: %d sd: %d\", x_mean, x_sd, y_mean, y_sd)\nlabel2 = sprintf(\"Sample size: %d, p-value: %g\", n, test$p.value)\nggarrange(pop, box, labels = c(label1, label2), font.label = list(size = 7))\n\n\n\n\nNon-normal\nNext I ran a simple set of scenarios to see how the intuition holds up if the true distributions are not normal. In these cases I compared the boxplot intuition against the Mann-Whitney U Test.\nTwo poisson distributions, same lambda\nIn this scenario I tested to see if the intuition and the test would correctly identify two samples coming from the same distributions. Both correctly failed to support the alternative hypothesis.\n\n\nx_mean <- 1 \ny_mean <- 1\nx <- rpois(1000, lambda  = x_mean)\ny <- rpois(1000, lambda  = y_mean)\n\nn <- 10\nx_samples <- sample(x, n)\ny_samples <- sample(y, n)\n\npop <- population_plot(x,y) %>% \n  add_samples(x_samples, y_samples)\nbox <- boxplot(x_samples, y_samples)\n\ntest <- wilcox.test(x_samples, y_samples)\n\nlabel1 = sprintf(\"x mean: %d, y lambda: %d\", x_mean, y_mean)\nlabel2 = sprintf(\"Sample size: %d, w p-value: %g\", n, test$p.value)\nggarrange(pop, box, labels = c(label1, label2), font.label = list(size = 7))\n\n\n\n\nTwo poisson distributions, different lambdas\nUsing samples from different distributions didn’t fool the test or the intuition either!\n\n\nx_mean <- 1 \ny_mean <- 3\nx <- rpois(1000, lambda  = x_mean)\ny <- rpois(1000, lambda  = y_mean)\n\nn <- 10\nx_samples <- sample(x, n)\ny_samples <- sample(y, n)\n\npop <- population_plot(x,y) %>% \n  add_samples(x_samples, y_samples)\nbox <- boxplot(x_samples, y_samples)\n\ntest <- wilcox.test(x_samples, y_samples)\n\nlabel1 = sprintf(\"x mean: %d, y lambda: %d\", x_mean, y_mean)\nlabel2 = sprintf(\"Sample size: %d, w p-value: %g\", n, test$p.value)\nggarrange(pop, box, labels = c(label1, label2), font.label = list(size = 7))\n\n\n\n\nConclusions\nOn the whole, I think the boxplot intuition isn’t awful. Unless twitter throws a revolt I may even stick with it.\nThe biggest failure I observed is respecting sample size. The boxplot intuition does not increase if the sample size is bigger, nor does it always reflect the uncertainty in a small sample.\nI was also pleasantly surprised that the intuition also held for at least one non-normal distribution.\nNatural Next Steps\nRun more simulations.\nAttempt different non-normal distributions.\nLook into multi-category testing.\n\nThis is probably a cool case of comparing cdfs to moments?↩︎\na much better name IMO than alternative↩︎\n",
    "preview": "posts/2021-06-18-boxplots-and-p-values/boxplots-and-p-values_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2024-04-23T20:22:42-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/hunt-for-the-pangrams-part-2/",
    "title": "Hunt for the Pangrams Part 2",
    "description": "More OCR in a quest for New York Times Spelling Bees",
    "author": [
      {
        "name": "Sean Lopp",
        "url": {}
      }
    ],
    "date": "2021-06-04",
    "categories": [],
    "contents": "\nIn my last post I started work to create a dataset that compiles information from the NYTimes Spelling Bee. The work essentially parses this excellent fan website to create a tidy dataset. I discoverd in my last post that parsing an image to get information on the daily game board was a little tricky. In this post I talk about how I fixed the OCR process and then look at some of the results.\nUpdating my Image Processing Routine\nMy last post discovered that a naive use of OpenCV and pytesseract was not going to correctly parse my images, which look like this:\ngameboardIn particular, the code was struggling to identify circular letters O and Q, it was failing to find X, and it was finding too many letters in some cases.\nTo rectify this issue I did three things:\nI used my data analysis to identify a few solid test images\nI switched from using RStudio as a Python editor to VS Code to take advantage of a better debug workflow\nI ended up tuning my cropping strategy and hard-coded a few image / classification rules\nI should have mentioned that in my first post I was using RStudio for all of my R and Python development. I wanted to see how far RStudio could take me in Python development. The result was pleasant, RStudio provides a Python environment pane for viewing objects as you execute code. This environment pane is really handy for interactive data science tasks, because it allows you to look at objects without requiring a special debug mode. However, in my case, the core part of my code was a loop:\nfor contour in contours:\n      # select cropped section\n      # classify cropped section as letter\nTo debug what was going wrong with certain characters I needed a fast and easy way to pause this loop and inspect the cropped image, the prediction, etc. This type of workflow is exactly what debuggers were built for, and in my case, that meant turning to VS Code because RStudio does not currently have a Python debugger.\nOnce I was setup with a debugger and test cases, it was relatively easy to see why classification was failing for certain characters. I realized right away that I had made a mistake in my original code. I was cropping and classifying the original image, even though I had generated contours off of a grayscale transformation of the image. Fixing that problem eliminated some of the cases where OCR found too many characters, and it allowed tesseract to correctly identify X. In my original code I had used a simple rule to determine if a contour was a character or another part of the image. I solved another set of my test cases by making this rule more precise to eliminate contours that were detecting partial characters within other characters. Finally, I discovered that the OpenCV contour, which I was using to crop one character out of my larger image, was not working well for O and Q.\nAt this point I had an interesting thought. In my case, all of my images were essentially the same. The letters were positioned exactly in the same locations, and the 26 letters were always the same whenever the appeared in the image. This realization meant I didn’t actually need to be doing ML at all…I decided to take this shortcut by saving samples of O and Q. I then added a simple case statement to check my cropped image against those two samples, providing 100% accuracy for my classification of those two letters. I could have followed the same procedure for the other 24 letters, but at this point the ML approach was working for everything else and I decided to let it be.\nThe resulting code can be seen in the post’s github repo: https://github.com/slopp/nytbee. A summary of the core OCR algorithim is:\n# use contours to dissect the parts of our image\ncontours,_=cv2.findContours(gray, cv2.RETR_TREE,\n                              cv2.CHAIN_APPROX_SIMPLE)\n                              \nocr_letters = list()\n\nfor cnt in contours :\n    # for each countour rough out the area \n    # we know the approx size of our letters, so ignore everything else\n    area = cv2.contourArea(cnt)\n    \n    \n    if area <300 and area>50:\n      # if the contour looks like a letter and is the right size, creating a cropped\n      # image that contains just the character\n      x, y, w, h = cv2.boundingRect(cnt)\n      cropped = gray[y:y + h, x:x + w]\n      \n      # if the cropped image matches an O or a Q, assign the label\n      if np.array_equal(cropped, o_img):\n        ocr_letters.append('O')\n      elif np.array_equal(cropped, q_img):\n        ocr_letters.append('Q')\n      else:\n      # otherwise, have pytesseract tell us what the image is\n        ocr_letters.append(pytesseract.image_to_string(cropped, config=\"--psm 10\"))\nPangram Analysis\nWith my data corrected, I was able to return to my original analysis. Are there certain letters, required letters, or genius scores that are predictive of the number of pangrams?\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\ndata <- readr::read_csv('nytbee.csv')\n\ndata <- data %>% \n  group_by(date) %>% \n  count() %>% \n  right_join(data) %>% \n  rename(total_letters_found = n) %>% \n  select(-X1)\n\nhead(data)\n\n\n# A tibble: 6 x 8\n# Groups:   date [1]\n  date     total_letters_f… num_pangram max_score max_words min_genius\n  <chr>               <int>       <dbl>     <dbl>     <dbl>      <dbl>\n1 Friday,…                7           1        92        37         64\n2 Friday,…                7           1        92        37         64\n3 Friday,…                7           1        92        37         64\n4 Friday,…                7           1        92        37         64\n5 Friday,…                7           1        92        37         64\n6 Friday,…                7           1        92        37         64\n# … with 2 more variables: letters <chr>, req_letter <chr>\n\nUsing this data we can learn one thing… games with many pangrams are outliers, and predicting outliers is hard.\n\n\ndata %>% \n  ggplot() + \n  geom_histogram(aes(num_pangram)) + \n  theme_minimal() +\n  labs(\n    x = \"Number of Pangrams\",\n    y = \"Games with that Many Pangrams\",\n    title = \"Games with more than 3 pangrams are extreme outliers\"\n  )\n\n\n\n\n\n\ndata %>% \n  select(-letters) %>% \n  unique() %>% \n  ggplot(aes(x = reorder(req_letter, num_pangram), y =num_pangram)) +\n    geom_boxplot() +\n    coord_flip() + \n  theme_minimal() + \n  labs(\n    x = \"Required Letter\",\n    y = \"Number of Pangrams\",\n    title = \"No required letter is a greater predictor of the number of pangrams\"\n  )\n\n\n\n\n\n\ndata %>% \n  unique() %>% \n  ggplot(aes(x = reorder(letters, num_pangram), y =num_pangram)) +\n    geom_boxplot() +\n    coord_flip() + \n  theme_minimal() + \n  labs(\n    x = \"Letter\",\n    y = \"Number of Pangrams\",\n    title = \"Non-required letters aren't good pangram predictors either\"\n  )\n\n\n\n\nWe can refine this view a bit with a model:\n\n\nlibrary(tidymodels)\nlm_mod <- \n  logistic_reg()\n\ndata$num_pangram <- as.factor(data$num_pangram)\n                              \nlm_fit <- \n  lm_mod %>% \n  fit(num_pangram ~ min_genius + req_letter + letters, data = data)\nviews <- tidy(lm_fit)\n\nviews %>% \n  arrange(desc(estimate))\n\n\n# A tibble: 47 x 5\n   term        estimate std.error statistic    p.value\n   <chr>          <dbl>     <dbl>     <dbl>      <dbl>\n 1 lettersQ       1.86      1.59      1.17  0.242     \n 2 req_letterW    1.82      0.386     4.72  0.00000231\n 3 req_letterK    1.26      0.376     3.35  0.000795  \n 4 req_letterD    0.915     0.276     3.31  0.000936  \n 5 req_letterU    0.661     0.311     2.12  0.0336    \n 6 req_letterY    0.661     0.312     2.12  0.0340    \n 7 req_letterB    0.503     0.278     1.81  0.0702    \n 8 req_letterH    0.387     0.339     1.14  0.253     \n 9 req_letterP    0.280     0.252     1.11  0.265     \n10 req_letterF    0.189     0.455     0.416 0.678     \n# … with 37 more rows\n\nSummary\nThese required letter suggest you should be on the pangram hunt: W,K,D\nThe higher the genius score the higher the number of pangams, but only marginally\nThe required letter tells you more about the potential for pangrams, but some letters are slightly predictive of more pangrams if they show up in the game even if they are not required. Letters to watch for still include W and K, but also U and H.\nQ is an interesting letter to model since it shows up rarely. It may suggest a higher number of pangrams, but the statistical evidence is weaker\n\n\n\n",
    "preview": "posts/hunt-for-the-pangrams-part-2/distill-preview.png",
    "last_modified": "2024-04-23T20:22:42-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/hunt-for-the-pangrams/",
    "title": "Hunt for the Pangrams",
    "description": "An accidental dive into OCR in a quest for New York Times Spelling Bees",
    "author": [
      {
        "name": "Sean Lopp",
        "url": {}
      }
    ],
    "date": "2021-05-31",
    "categories": [],
    "contents": "\nCode for post at https://github.com/slopp/nytbee\nThe New York Times Spelling Bee is a game where players try to spell valid words using 7 letters. The 7 letters can be repeated any number of times. In each game, one letter is required and must be included in any word. In each game there is at least 1 pangram, which is a word that includes all 7 letters at least once.\nPlayers get points based on the length of each word they can spell. Pangrams are worth extra points, and for me, are the most exciting part of the game. However, the game does not tell you how many pangrams exist in any given puzzle.\nThe goal of this analysis is to see if I can determine a model that will predict the number of pangrams given information I have at the start of each game. Part of that information is the game board itself, including the letters and the required letter used in the puzzle. The other piece of information is the genius score.\nWhat is the genius score?\nBecause some puzzles are not as conducive to creating words, it is not possible to compare one game to another based solely on point totals. Instead, the game makers apply a normalizing function that translates scores into categories based on the max possible score. For example, in the spelling bee on 5/31/2021, the rankings and associated minimum required points were:\nBeginner (0)\nGood Start (5)\nMoving Up (13)\nGood (22)\nSolid (40)\nNice (67)\nGreat (108)\nAmazing (135)\nGenius (188)\nMy goal is to see if I can determine the number of pangrams, \\(p\\), based on the point total required for the genius category \\(g\\) plus information about the letters in the game, \\(l\\). In other words:\n\\[\n\\hat{p} = \\hat{h}(g, l)\n\\]\nGet the data\nA generous spelling bee enthusiast maintains a website https://nytbee.com that contains helpful information about each daily bee. Looking at the site before starting the game is one way to determine the number of pangrams in play… however, I believe looking at this site before playing a puzzle is akin to cheating! That said, the daily information is invaluable to creating my model. Historical data is available at links formatted like: https://nytbee.com/Bee_yyyymmdd.html\nMy first task was to create a tidy dataset based on parsing these web pages. This turned out to be quite an involved task:\nUsed beautiful soup in Python to parse each URL to get the metadata\nCreated an OCR workflow, also in Python, to parse the letters from the static image of the game board\nThe details of this workflow are detailed at the bottom section of this post. For now, let’s take a quick look at the result of this workflow:\n\n\n# run to create the dataset, takes a few minutes\n# reticulate::source_python('ocr.py')\n\ndata <- readr::read_csv('nytbee.csv')\n\n# add a column counting how many letters we claim to have for each day\n# remember, each day should only have 7 letters\n# result: oh no\ndata <- data %>% \n  group_by(date) %>% \n  count() %>% \n  right_join(data) %>% \n  rename(total_letters_found = n) %>% \n  select(-X1)\n\n# oh no\nhist(data$total_letters_found)\n\n\n\n# less than half the days correctly have 7 letters\ndata %>% \n  filter(total_letters_found ==7) %>% \n  group_by(date) %>% \n  n_groups()\n\n\n[1] 142\n\n# what went wrong?\n# result: letters we are missing: O Q X\ndata %>% \n  pull(letters) %>% \n  unique() %>% \n  setdiff(LETTERS, .)\n\n\n[1] \"I\" \"O\" \"Q\" \"X\"\n\n# we shouldn't have more than one of any letter\n# in a given day per game rules\n# result: this validation was successful\ndata %>% \n  group_by(date, letters) %>% \n  count() %>% \n  filter(n > 1)\n\n\n# A tibble: 0 x 3\n# Groups:   date, letters [0]\n# … with 3 variables: date <chr>, letters <chr>, n <int>\n\n# so what extra letters are being identified?\n\n# lets look at letters that show up in cases where wemore than 7 are found\n# result: we have many A and S, so those could be mis-identified or maybe those are just common?\ndata %>% \n  filter(total_letters_found > 7) %>% \n  group_by(letters) %>% \n  count() %>% \n  arrange(desc(n))\n\n\n# A tibble: 23 x 2\n# Groups:   letters [23]\n   letters     n\n   <chr>   <int>\n 1 A         118\n 2 S         118\n 3 |          78\n 4 L          61\n 5 T          57\n 6 N          56\n 7 C          54\n 8 E          48\n 9 Y          39\n10 H          36\n# … with 13 more rows\n\n# what are A and S correlated with?\n# result: they could be confused with L, N, |, T, C, E, R\na_s_days <- data %>% \n  filter(letters %in% c(\"S\", \"A\")) %>% \n  pull(date)\n\n\n\nEnd result:\nMy OCR tool is not quite good enough. Out of 365 days, it produced 142 accurate datasets with 7 letters, less than 50% of the attempted 365 days! Of the inaccuracies, the biggest is missing letters: it did not identify any O, Q, or Xs. These letters have a major impact on game scoring, so any model of the game would have significant bias without days that included these letters. The days with too many letters are equally troubling. In addition to the wrong letters, my determination of the required letter was based on the OCR analysis recording identifying letters in a consistent order. If a letter can’t be found, this messes with the order, and prevents the data about the required letter from being accurate. In other words, where the total_letters_found != 7, it is safe to assume the required letter is off as well.\nSafe analysis despite data prep failures\nBefore completely calling it, we can used the data we successfully parsed off the HTML pages (not the image data) to see if there is a simple relationship between the genius score and the number of pangrams:\n\n\ndata %>% \n  select(-letters) %>% \n  unique() %>% \n  ggplot(aes(min_genius, num_pangram)) + \n  geom_jitter(alpha = 0.3) + \n    geom_smooth() + \n    labs(\n      title = \"You can't use the genius score to predict the # of pangrams\",\n      subtitle = \"Unless the score is north of 250, in which case you can look for at least 3\",\n      y = \"Number of Pangrams\",\n      x = \"Min Points to be Genius\"\n    ) + \n    theme_minimal() \n\n\n\n\nThere is not a great relationship. There are a lot of games where there is only one pangram and the scores in those games varies significantly! This chart shows us that there are some outlying games with more than 4 pangrams, and that those mega-games tend to be correlated with high scores. But score alone won’t help you guess whether the game has 1, 2, 3, or 4 pangrams.\nWe can tell there is not a statistically significant difference if we compare the IQR within the box plots:\n\n\nggplot(data, aes(as.factor(num_pangram), min_genius)) + \n  geom_boxplot() + \n  labs(\n    x = \"Number of Pangrams\",\n    y = \"Min Points to be Genius\",\n    title = \"Games with 1-4 pangrams can have similar point totals\"\n  ) + \n  theme_minimal()\n\n\n\n\nFinally, we can use the scraped data to double check that the game makers fairly assign the minimum genius score, by looking at the percentage of total points you have to get to be considered a genius:\n\n\ndata %>% \n  mutate(perc_genius = min_genius / max_score) %>% \n  ggplot(aes(perc_genius)) + \n  geom_histogram() + \n  theme_minimal() + \n  labs(\n    x = \"Percentage of Points Needed to be Genius\",\n    y = \"# of Games\",\n    title = \"The genius score is roughly 70% of the max game points\",\n    subtitle = \"C- is Genius?!\"\n  )\n\n\n\n\nThis histogram shows that the game maker are very accurate, as the percentage of points required is a fixed 70%. 70% to be a genius? Nice. I am guessing the variance here is that each word has an integer number of points, so its not always possible to have exactly 70% of the max point total. We would NOT want to ever fit a model that includes max point and genius points since the two are almost 100% colinear. What about the relationship between max points and number of words?\n\n\nggplot(data, aes(max_words, min_genius)) + \n  geom_point() + \n  geom_smooth() + \n  theme_minimal() + \n  labs(\n    x = \"Max # of Words\",\n    y = \"Min Genius Score\",\n    title = \"The possible score is related to the number of words you can make\",\n    subtitle = \"This relationship would be great for pangram models, but we don't know the # of words ahead of playing\"\n  )\n\n\n\n\nAs expected, given how the scoring works, the score (proxied by the minimum genius score) is closely correlated with the maximum number of words, but there is some variance. In fact, discrepancies between this relationship might point at games with a large number of pangrams! However, we don’t know the number of words going into a game, so this information wouldn’t help a player identify how many pangrams there may be. That said, we can safely assume that if genius score is high, the puzzle likely has more words!\nFinally, one last area of investigation we could safely look at with our data is how the occurence of pangrams have changed over time. Are we getting more pangrams?\n\n\ndata %>% \n  group_by(date) %>% \n  filter(num_pangram > 1) %>% \n  select(-letters, -req_letter, -total_letters_found) %>% \n  unique() %>% \n  arrange(desc(num_pangram))\n\n\n# A tibble: 78 x 5\n# Groups:   date [78]\n   date                     num_pangram max_score max_words min_genius\n   <chr>                          <dbl>     <dbl>     <dbl>      <dbl>\n 1 Friday, January 22, 2021           7       537        75        376\n 2 Monday, May 24, 2021               6       406        74        284\n 3 Wednesday, May 5, 2021             6       390        66        273\n 4 Thursday, February 11, …           5       392        68        274\n 5 Friday, December 4, 2020           4       186        38        130\n 6 Monday, May 31, 2021               4       269        53        188\n 7 Sunday, May 9, 2021                4       360        68        252\n 8 Sunday, September 20, 2…           4       213        37        149\n 9 Monday, February 1, 2021           3       379        72        265\n10 Saturday, August 8, 2020           3       249        46        174\n# … with 68 more rows\n\nTo fairly do this analysis we’d want to pull more data, but of the 22 puzzles with more than 3 pangrams, 13 occurred in 2021 vs 9 in 2020. While not a dramatic increase YoY, all of the really outlying pangram games have been in 2021. Its a good year for pangram hunters.\nSketchy and biased analysis\nUsing the biased data that I had, I was curious if there was any pattern between a required letter and the likelihood of more than 2 pangrams? In other words, if I see a certain required letter, should I be on the pangram hunt?\n\n\ndata %>%\n  filter(total_letters_found == 7) %>% \n  select(-letters) %>% \n  unique() %>% \n  group_by(req_letter) %>% \n  filter(num_pangram > 2) %>% \n  count() %>% \n  arrange(desc(n)) %>% \n  rename(`Number of games > 2 pangrams` = n)\n\n\n# A tibble: 3 x 2\n# Groups:   req_letter [3]\n  req_letter `Number of games > 2 pangrams`\n  <chr>                               <int>\n1 T                                       4\n2 Y                                       2\n3 K                                       1\n\nReally interesting! I would have expected more pangrams if the required letter was a vowel.\nI looked at some of the specific date where OCR failed, especially our outlier day of Jan 22, 2021, and low and behold, these mis-placed days often DID have vowels as the required letter. Our conclusions here are sketchy. Garbage in, garbage out. Always get your data prep right!\nJust how biased is our missing data w.r.t pangrams?\n\n\ndata %>%\n  select(-letters) %>% \n  unique() %>% \n  mutate(bad_data = ifelse(total_letters_found == 7, FALSE, TRUE)) %>% \n  ggplot() + \n  geom_bar(aes(as.factor(num_pangram), fill = bad_data), position = \"dodge\") + \n  labs(\n    x = \"Number of Pangrams\",\n    y = \"Number of Games\",\n    fill = \"Affected by Bad OCR\"\n  ) + \n  scale_fill_brewer() + \n  theme_minimal()\n\n\n\n\nMore on the data prep\nSo what is the deal with OCR? Each of the nytbee webpages with historical spelling bee data has an image that represents the game board:\n\nMy goal in doing the data preparation was to parse all of the historical webpages, and as part of the parsing, convert this image into tidy data. I wanted to know the letters available on the board each day, along with the required center letter.\nIt turns out, parsing this image was harder than expected. My ultimate strategy was to use OpenCV to find the letters in the inage, and then use tesseract to identify the character. I’ll walk through the Python code below, including helpful images along the way.\nI used a Python container running in Kubernetes for my work. The container hepled me easily create a virtualenv and a system environment with the proper OpenCV and Tesseract dependencies, and Kubernetes was just what I had around for running the container on a nicely networked EC2 instance.\n# first load the img\nimport skimage\nimport cv2\nurl = \"https://nytbee.com/pics/20210531.png\"\nimg = io.imread(url)\n# next identify the countours\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n# find the contours\ncontours,_=cv2.findContours(gray, cv2.RETR_TREE,\n                            cv2.CHAIN_APPROX_SIMPLE)\n                            \n                            \n# Loop through the contours... I used a trick to separate the letter contours from the polygon contours based on their area\n# for the first loop we'll eliminate the polygon lines\nimg2 = img.copy()\nfor cnt in contours :\n      # for each countour rough out the area size\n      area = cv2.contourArea(cnt)\n      \n      # if the area is large its a polygon not a letter, so flag it\n      if area > 400: \n        approx = cv2.approxPolyDP(cnt, \n                                  0.009 * cv2.arcLength(cnt, True), True)\n        cv2.drawContours(img2, [cnt], 0, (255,255,255), 4)\n\ncv2.imwrite(\"polygons.png\", img2)\n\n# now we loop through the smaller \"letter\" polygons\n# here I will show you the boxes\nfor cnt in contours :\n      # for each countour rough out the area size\n      area = cv2.contourArea(cnt)\n      \n      # if the area is small zoom in on it as a letter\n      if area < 300: \n         x, y, w, h = cv2.boundingRect(cnt)\n         rect = cv2.rectangle(img2, (x, y), (x + w, y + h), (0, 255, 0), 2)\n        \n\ncv2.imwrite(\"letters.png\", img2)\n\n# finally, this time as we loop through, we'll run the predictions using tesseract\nocr_letters = list()\nfor cnt in contours :\n    # for each countour rough out the area size\n    area = cv2.contourArea(cnt)\n    \n    # if the area is small, then figure out the letter!\n    if area <300:\n      x, y, w, h = cv2.boundingRect(cnt)\n      cropped = img2[y:y + h, x:x + w]\n      ocr_letters.append(pytesseract.image_to_string(cropped, config=\"--psm 10\"))   \n['H\\n\\x0c', '|\\n\\x0c', 'G\\n\\x0c', 'T\\n\\x0c', '[L\\n\\x0c', 'E\\n\\x0c', 'D\\n\\x0c']\nTo view the full prep code, take a look at ocr.py in the GitHub repository. The letters are mostly correct, we just need to clean up the predictions a bit.\nAt this point though, it’d be good to understand why we are doing so poorly on some images! And that is a post for another holiday…\n\n\n\n",
    "preview": "posts/hunt-for-the-pangrams/distill-preview.png",
    "last_modified": "2024-04-23T20:22:42-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-05-31-observable-for-r/",
    "title": "Observable for R",
    "description": "What R users should know about Observable notebooks",
    "author": [
      {
        "name": "Sean Lopp",
        "url": {}
      }
    ],
    "date": "2021-03-23",
    "categories": [],
    "contents": "\nThis is an embedded notebook. Head on over to observable to view the real editing experience including the ability to fork or comment.\nNotebook at: https://observablehq.com/@slopp/observable-for-r-users\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2024-04-23T20:22:42-06:00",
    "input_file": {}
  },
  {
    "path": "posts/tasty-models/",
    "title": "Tasty Models",
    "description": "Different feature engineering approaches using TidyTuesday's Chopped data",
    "author": [
      {
        "name": "Sean Lopp",
        "url": {}
      }
    ],
    "date": "2020-11-15",
    "categories": [],
    "contents": "\nIntro\nI recently discovered a popular set of Python packages for automated feature generation created by FeatureLabs, a MIT spin off that is now a part of Alteryx.\nThe core package is called featuretools, which generates automated features and is well suited to time series classification problems; e.g. “which customers will churn this month?” or “which wind turbine will fail next week?”.\nI found their blogs a bit “markety” - they seem to be fond of coining terms that make relatively simple concepts like “write parameterized functions” sound fancy: “prediction engineering”. But perhaps this coining is best interpreted as ML salesmanship? Either way, I digress. The package has excellent reviews and appears to be the real deal (read: supported and scalable).\nIn researching the Python package I found an excellent reticulate-based implementation in R called featuretoolsR. Finding the R package inspired me to give the framework a spin. It was also a good excuse to explore a fun TidyTuesday dataset from earlier this year, Chopped episode reviews!\n\nA post on feature engineering that uses data from a show built around ingredients… let the analogies and puns begin!\n\nAs I played more with the data I ended up generating all kinds of features in a variety of different ways. This post explores the following:\nA time series regression problem: can we predict the next episode’s rating? Spoiler: not really, the end models I created were not great.\nFeature engineering: features built manually, features from recipes, and features from featuretoolsR.\n\n\n╔═════════════════════════╗\n║   featuretoolsR 0.4.4   ║\n╚═════════════════════════╝\n✓ Using Featuretools 0.21.0\n\nThe Chopped data consists of a time series (episodes aired on a certain date), episode ratings (the quantity to predict), and a variety of episode data well suited to generating features.\n\n\npaged_table(chopped_rating)\n\n\n\nTo begin the exploratory analysis, I took a look at the ratings across shows. (I removed any shows with no rating when I loaded the data).\n\n\nggplot(chopped_rating) + \n  geom_histogram(aes(episode_rating)) + \n  labs(\n    title = \"Predicting Chopped Episode Ratings\",\n    y = NULL,\n    x = \"Rating\"\n  )\n\n\nAs expected, Chopped had generally stellar ratings!\nFeatures based on EDA\nNext, I wanted to take a look at some of the features in the data to continue the exploratory analysis and see if any attributes may have the potential to predict episode rating.\nEDA by Season\nOne place to start is to see if episode rating varied by season. Who knew Chopped has 43 seasons?!\n\n\nchopped_season <- chopped_rating %>% \n  group_by(season) %>% \n  summarize(episode_count = n())\nchopped_rating %>%\n  ggplot() + \n    geom_boxplot(aes(x = as.factor(season), y =episode_rating)) + \n    geom_smooth(aes(x = season, y = episode_rating)) + \n    geom_text_repel(data = chopped_season, aes(season, 6, label=episode_count)) + \n  labs(\n    x = 'season'\n  )\n\n\nIn this case it appears the ratings decreased gradually overtime, with the exception of the final seasons which had much fewer episodes with rating data. It is possible that incorporating season into the model could help predict the episode rating.\nEDA by Judge\nNext I looked at ratings by judge. I have favorite judges afterall, maybe most people do?\n\n\nby_judge <- chopped_rating %>% \n  select(episode_rating, season, starts_with(\"judge\")) %>% \n  pivot_longer(starts_with(\"judge\")) %>% \n  group_by(value) %>% \n  mutate(avg = mean(episode_rating),\n         appearances = n()) \n\nby_judge %>% \n  ggplot(aes(reorder(value, avg), episode_rating)) + \n  geom_boxplot() + \n  coord_flip()\n\n\nThis plot is a bit hard to read, but it does suggest a few things:\nThere are some judges who have only done one episode. This observation makes “judge” a hard variable to use for predictions for two reasons. For existing data, many guest judges represent singular values, no predictive insight there. For out of sample data, it is likely we would see new judges we know nothing about, also not helpful for predictions.\nThere may be differences between the recurring judges who have been on many episodes. Let’s take a deeper look:\n\n\ninfluential_judges <- by_judge %>% \n  select(value, avg, appearances) %>% \n  unique() %>% \n  filter(appearances > 10) %>% \n  pull(value)\n\nby_judge %>% \n  filter(value %in% influential_judges) %>% \n  ggplot(aes(reorder(value, avg), episode_rating)) + \n  geom_boxplot() + \n  coord_flip()\n\n\nThis plot suggests that the “recurring” judges all have ranges that are pretty similar (all the boxplots are overlapping), which means the specifics of a recurring judge may not be that predictive. However something may be better than nothing, so I figured it would be worthwhile to add judges as a feature. How? One approach is create a set of dummy variables representing the recurring judges on each show:\n\n\nencode_judge <- function(judges) {\n  judges\n  encode = tibble(\n    judge = influential_judges,\n    in_episode = 0\n  )\n  encode %>% \n    mutate(in_episode = judge %in% judges) %>% \n    pivot_wider(names_from = judge, values_from = in_episode)\n}\n\nchopped_rating[1:10,] %>%\n  rowwise() %>% \n  mutate(encode_judge(c_across(starts_with(\"judge\")))) %>% \n  paged_table()\n\n\n\nOne thing to note about this approach; we essentially had to run a “forward pass” across all of our data to compute the list of influential judges. The encoding then uses that fixed list. While this approach works, it is also risky because future data might drift - the judges might stop judging and new judges (not encoded here) could become part of the data! A similar problem occurs whenever you are training factors on data where new factors could potentially come into play.\nThis encoding function creates dummy variables across all the possible influential judge outcomes. Another approach would have been to create 3 variables: judge1, judge2, and judge3, and have each be a factor. In this case I went for the dummy encoding because I know judges are mutually exclusive (judge1 != judge2).\nEDA Entity Extraction\nThere are a few variables in our data that are quite wordy. We may want to take, for instance, the episode notes and create a sentiment score (did something bad happen in highly rated episodes?). Or we could try to extract entities like title or location from the contestant info. Or we could play with the ingredients, for instance, to see if a particular ingredient was most often associated with high ratings. So many options!\nI decided to start with _info fields, which contain free text information about the contestants:\n\n\nchopped_rating %>% \n  select(ends_with('info')) %>% \n  paged_table()\n\n\n\nSome of this information may be interesting for the model, but it is unlikely that the free form fields themselves will be useful. The info appears to commonly include location and title. These two bits can be extracted in a NLP process called entity extraction. Think of NLP entity extraction as a magic mutate call that can take free form text and pull out values of interest such locations.\n\n\nlibrary(spacyr)\n# this launches the python session where the spacy nlp functions\n# actually run. hooray reticulate!\nspacy_initialize()\n\n# lets take a look at just the first contestants\nentities <- spacy_extract_entity(chopped_rating$contestant1_info)\npaged_table(entities)\n\n\n\nIt looks like spacy is correctly identifying cities (GPE is short for geopolitial entity), but is getting confused by titles and persons. I’ll take what I can get, so lets create a function that will pull out cities. Unlike judges, I will create this encoding so that each contestant’s city is represented in a column contestant1_city, contestant2_city… Then later on we will use a recipe step to turn these values into factors. I use this encoding approach here because there can be multiple cities represented in the same show (in fact that is quite common).\n\n\nencode_cities <- function(chefs_info){\n  # attempt to extract entities from the info for all chefs in a given episode\n  entities <- map_df(chefs_info, spacy_extract_entity, .id = 'contestant')\n  if(\"ent_type\" %in% colnames(entities)) {\n    # pick the first city entity for each contestant\n    identified <- entities %>% \n     filter(ent_type == \"GPE\") %>% \n     select(contestant, text) %>% \n     group_by(contestant) %>% \n     summarize(text = first(text))\n  } else {\n    identified <- tibble(contestant = NA, text = NA)\n  }\n    # we may not have found entities, so add defaults to be sure\n    # we always have a column for each contestant \n    defaults <- tribble(\n      ~contestant, ~text,\n      \"1\", NA,\n      \"2\", NA,\n      \"3\", NA,\n      \"4\", NA\n    )\n    results <- left_join(defaults, identified, by=c(contestant = \"contestant\")) %>% \n      mutate(location = coalesce(text.x, text.y)) %>% \n      select(contestant, location) %>% \n      pivot_wider(\n        names_from = contestant,\n        names_prefix = \"city_\",\n        values_from = location\n      )\n    return(results)\n}\n\nchopped_rating[1:10,] %>%\n  # pull out cities\n  rowwise() %>% \n  mutate(encode_cities(c_across(ends_with(\"info\")))) %>% \n  paged_table()\n\n\n\nEDA Sentiment\nAnother field that might be of interest is the episode notes. While these notes are highly variable, the sentiment of the notes might be interesting. Are neutral shows boring? Are positive or negative notes revealing of a more interesting show?\n\n\nlibrary(tidytext)\nsentiments <- get_sentiments(\"afinn\")\nseries_sentiments <- chopped_rating %>% \n  unnest_tokens(word, episode_notes) %>% \n  left_join(sentiments) %>% \n  group_by(series_episode) %>% \n  summarize(note_sentiment = mean(value, na.rm = TRUE))\n\nchopped_with_sentiments <- chopped_rating %>% \n  left_join(series_sentiments)\n\nggplot(chopped_with_sentiments) + \n  geom_jitter(aes(episode_rating, note_sentiment))\n\n\nAdd EDA Features\nWe can now finally add all these features we’ve built through our traditional EDA based approach:\n\n\nchopped_plus_eda <- chopped_with_sentiments %>%\n  # add judges\n  rowwise() %>% \n  mutate(encode_judge(c_across(starts_with(\"judge\")))) %>% \n  # entity extraction for cities\n  mutate(encode_cities(c_across(ends_with(\"info\"))))\n\nAutomated Features with featuretools\nOur next step is do some automated feature generation using featuretools!\nThe package is designed to help automate feature creation through a process called “deep feature synthesis”. In my opinion this title is designed to make a pretty simple concept sound fancy. The core idea in featuretools is that you can take primitive operations to create new features, and then sometimes you can combine those operations to create even more new features.\nFor example, in our Chopped data set you could take primitives like “week_day” and “mode” to generate a number of new feature columns: week_day(episode), mode(season_rating), and then combinations like: mode(week_day(episode)).\nThere are two types of primitives, “transform” and “aggregation”. For tidyverse fans, an easy way to understand the difference is to think of the dplyr functions mutate and summarize. Transform primitives are functions that fit within a mutate; they return one value for each row. Aggregation primitives, on the other hand, require some type of grouping. They are the equivalent of a summarize function. So, in our example above, which type is week_day and mode?\nDid you think about it? week_day is a transform primitive, it takes a date and for each date returns a weekday. mode is as an aggregation primitive, it returns the mode of an input column per group.\nNow the part that makes “deep feature synthesis” fancy is that combinations of the two types of primitives can also be used to generate new features. For example, mode(week_day()).\nLet’s apply the concept to our data:\n\n\n# convert date to date time for correct time handling below\nchopped_plus_eda$air_date <- mdy_hms(paste0(chopped_plus_eda$air_date, \"20:00:00\"))\n\n# first we have to create an entity, which is the building block of featuretools\nchopped_entity <- \n  as_entityset(\n    chopped_plus_eda,\n    index = \"series_episode\",\n    time_index = \"air_date\",\n    entity_id = \"Chopped\"\n  )\n\n# next we have to teach featuretools about the relationship between \n# seasons and episodes\nchopped_seasons <- tibble(\n  season = unique(chopped_plus_eda$season)\n)\n\nchopped_entity <- chopped_entity %>% \n  add_entity(\n    df = chopped_seasons,\n    entity_id = \"seasons\",\n    index = \"season\"\n  ) %>% \n  add_relationship(\n    parent_set = \"seasons\",\n    child_set = \"Chopped\",\n    parent_idx = \"season\",\n    child_idx = \"season\"\n  )\n\n# finally we have to be careful to handle time\n# we don't want featuretools to leak data\n# for instance, if we create a feature MAX(season.ingredients)\n# that feature must actually be a \"rolling\" max over the course\n# of the season, since we don't know how many ingredients a show in\n# the future would have\ncutoff_times <- chopped_plus_eda %>% \n  select(series_episode, air_date)\n\n# now we perform the automatic feature generation\nchopped_new_features_entity <- chopped_entity %>%\n  dfs(\n    target_entity = \"Chopped\", \n    trans_primitives = c(\"year\",\n                         \"time_since_previous\", \n                         \"num_characters\", \n                         \"week\", \n                         \"weekday\", \n                         \"month\", \n                         \"num_words\"),\n    agg_primitives = c(\"max\",\n                       \"mode\", \n                       \"num_unique\", \n                       \"mean\", \n                       \"sum\"),\n    max_depth = 3,\n    include_cutoff_time = TRUE,\n    cutoff_time = cutoff_times,\n    # be sure to ignore the episode_rating since that is our target\n    # we don't want it leaking into the generated features\n    # also ignore some variables we don't want to use for feature generation\n    ignore_variables = list(Chopped = list(\"episode_rating\",\n                                           \"contestant1\",\n                                           \"contestant2\",\n                                           \"contestant3\",\n                                           \"contestant4\",\n                                           \"contestant1_info\",\n                                           \"contestant2_info\",\n                                           \"contestant3_info\",\n                                           \"contestant4_info\",\n                                           \"episode_notes\"))\n  )\nchopped_new_features <- tidy_feature_matrix( chopped_new_features_entity)\n\n# add back our index\nchopped_all <- \n  chopped_plus_eda %>% \n  select(series_episode, \n         air_date, \n         episode_rating,\n         season,\n         season_episode) %>% \n  left_join(chopped_new_features, \n            by = c(season = \"season\", \n                   season_episode = \"season_episode\"))\n\npaged_table(chopped_all)\n\n\n\nWe created many new features, and then combinations of those features! Will these be useful for modeling? That is like asking whether you can have too many ingredients in your pantry… maybe yes, maybe no…\nRecipes\nNow that we have our giant set of features we are ready to create a recipe and do some modeling. For fun, we can even create a few more features along the way…\nOne critical bit as we build our recipe is to keep in mind the time element; just like in our automated feature engineering we need to be sure we don’t leak data. We also have to be careful about time when we make our test/training split - we wouldn’t want to accidentally train on the future and test on the past!\n\n\nlibrary(timetk)\nlibrary(tidymodels)\n\n# hold out some test data, going back\n# far enough to have test data from full seasons\nsplits <- initial_time_split(chopped_all, prop = 0.3)\ntrain_data <- training(splits)\ntest_data  <- testing(splits)\n\nIt was at this point I made an interesting discovery… the series_episode index did not always line up with the air_date.\nA digression…\nIn other words, there were cases like this:\n\n\nchopped_all %>% \n  filter(series_episode > 145, series_episode < 151) %>% \n  select(season, season_episode, series_episode, air_date, episode_name) %>% \n  paged_table()\n\n\n\nEssentially there appears to be overlap between the beginning of one season and the end of another. This makes sense, because there are 43 seasons over the course of only 11 years. We can look at some of the details:\n\n\nout_of_order = 0\nout_of_order_idx = rep(FALSE, nrow(chopped_all))\nfor(i in 1:nrow(chopped_all)) {\n  max_date <- max(chopped_all$air_date[1:(i-1)])\n  if(chopped_all$air_date[i] < max_date) {\n    out_of_order <- out_of_order + 1\n    out_of_order_idx[i] <- TRUE\n  }\n}\n\nchopped_all %>% \n  select(season, season_episode, air_date, series_episode) %>% \n  cbind(out_of_order_idx) %>% \n  paged_table()\n\n\n\nTotal out of order is 162. While surprising, I realized this out of order index didn’t make a huge difference, it just meant in order to avoid “leaking” data about the future, we would need to arrange by air_date when making test and training splits.\n\nNote to future self: don’t take time and monotonically increasing indexes for granted!\n\nReturning to the recipe and model fitting:\n\n\n# arrange by time\n# remove some variables that won't allow models to converge\n# because they have a 1:1 mapping with episode rating\n# fix the names created by feature tools or some models get upset\nchopped_all <- chopped_all %>%\n  arrange(air_date) %>% \n  select(-starts_with(\"judge\"),\n         -episode_name,\n         -series_episode) %>% \n  set_tidy_names(syntactic = TRUE)\n        \n  \n\n# hold out some test data, going back\n# far enough to have test data from full seasons\nsplits <- initial_time_split(chopped_all, prop = 0.7)\ntrain_data <- training(splits)\ntest_data  <- testing(splits)\n\n# verify no overlap this time\nmax(train_data$air_date)\n\n[1] \"2016-05-24 20:00:00 UTC\"\n\nmin(test_data$air_date)\n\n[1] \"2016-05-31 20:00:00 UTC\"\n\n# now we can create our recipe\n# our goal for the recipe is to specify the formula for our predictive problem\n# we also convert some of data to the appropriate type (factor, etc)\n# and impute missing values\nchopped_rec <- \n  recipe(episode_rating ~ ., data = train_data) %>%\n  step_num2factor(season, \n                  ordered = FALSE, \n                  levels = as.character(1:43)) %>% \n  step_num2factor(season_episode, \n                  ordered = TRUE, \n                  levels = as.character(1:20))  %>%\n  # this is a really important step\n  # it helps our model handle cases where new cities in the test data\n  # don't \"surprise\" the model, by assigning them automatically to a \n  # factor called \"other\"\n  # it also assigns any cities that appear < 3 times to \"other\"\n  step_other(all_nominal(), -season, -season_episode, threshold = 3) %>% \n  step_normalize(all_numeric(), -episode_rating) %>% \n  step_meanimpute(all_numeric(), -season_episode) %>% \n  step_zv(all_predictors())\n\n\n# take a look\nchopped_rec\n\nData Recipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         82\n\nOperations:\n\nFactor variables from season\nFactor variables from season_episode\nCollapsing factor levels for all_nominal(), ...\nCentering and scaling for all_numeric(), -episode_rating\nMean Imputation for all_numeric(), -season_episode\nZero variance filter on all_predictors()\n\nchopped_train_vals <- juice(prep(chopped_rec, train_data))\npaged_table(chopped_train_vals)\n\n\n\nModel Fitting\nNow that we have our recipe and an absurd number of variables, we can do our model fitting. The goal of this post is not to compare too many model structures, but to instead get a feel for whether our added features are worthwhile!\n\n\n# specify a random foerst model\nmod <- rand_forest(mode = \"regression\") %>% \n  set_engine(\"ranger\", importance = \"impurity\")\n\n# prepare our recipe\nchopped_rec_prepped <- prep(chopped_rec, train_data)\n\nmod_fit <- \n  fit(mod, episode_rating ~ ., data = bake(chopped_rec_prepped, train_data))\n\ntrain_res <- \n  predict(mod_fit, bake(chopped_rec_prepped, train_data)) %>% \n  bind_cols(train_data$episode_rating)\n\ntrain_res %>% \n  metrics(.pred, `...2`)\n\n# A tibble: 3 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       0.162\n2 rsq     standard       0.927\n3 mae     standard       0.120\n\ntest_res <- \n  predict(mod_fit, bake(chopped_rec_prepped, test_data)) %>% \n  bind_cols(test_data$episode_rating)\n\ntest_res %>% \n  metrics(.pred, `...2`)\n\n# A tibble: 3 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      0.579 \n2 rsq     standard      0.0291\n3 mae     standard      0.446 \n\nggplot(test_res, aes(.pred, `...2`)) + \n  geom_point() + \n  geom_abline(slope = 1, intercept = 0) + \n  labs(\n    x = \"Predicted\", \n    y = \"Actual\"\n  )\n\n\nThe results are bleak. While our model performs really well on the training data, it performs poorly on the test data. This behavior could be for a few reasons. I am suspicious we may be over-fitting based on our over-abundance of inputs relative to our training data. There may also be structural changes between our training data (early seasons) and our test data (later seasons). I also didn’t tune the model or think much about model structure!\nIt is always risky to assume more ingredients make for a better meal!\nModel performance aside, we can use feature importance for tree models to get a sense for which of the many features are making the biggest impact in the model.\n\n\nlibrary(vip)\nvip::vip(mod_fit)\n\n\nLooks like our auto-generated features came in handy, along with some of the EDA based work. As expected, the hard work we did extracting judges didn’t help the model much, whereas the features we got for free (season and air date) were important. The model did use our sentiment data, but didn’t do much with extracted cities. Interestingly, it appears both appetizers, entree, and dessert columns were useful thanks to our auto-generated features!\nWhat if the model only used features available in the original data set?\n\n\nmod_less <- rand_forest(mode = \"regression\") %>% \n  set_engine(\"ranger\", importance = \"impurity\")\n\nmod_less_fit <- \n  fit(mod_less, episode_rating ~ air_date + season + season_episode, data = bake(chopped_rec_prepped, train_data))\n\ntrain_less_res <- \n  predict(mod_less_fit, bake(chopped_rec_prepped, train_data)) %>% \n  bind_cols(train_data$episode_rating)\n\ntrain_less_res %>% \n  metrics(.pred, `...2`)\n\n# A tibble: 3 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       0.240\n2 rsq     standard       0.731\n3 mae     standard       0.180\n\ntest_less_res <- \n  predict(mod_less_fit, bake(chopped_rec_prepped, test_data)) %>% \n  bind_cols(test_data$episode_rating)\n\ntest_less_res %>% \n  metrics(.pred, `...2`)\n\n# A tibble: 3 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      0.620 \n2 rsq     standard      0.0717\n3 mae     standard      0.492 \n\nThe less complicated model doesn’t perform as well as the original model, but it does perform in the same ball park; a rmse of 0.62 vs 0.58 for the significantly more complicated model. Is the complexity worth it? In this case, most likely not. Perhaps the lesson here is that model fitting really is like cooking on Chopped - keep things simple, focus on your main ingredients, and then execute.\nNotes on Production Pipelines\nFor future out of sample predictions we’d have to generate a pipeline to create all our features since our current recipe doesn’t capture all the things we did to prep our data. As mentioned in the EDA section, we would need to be careful to watch for never before seen “influential” judges as well as new cities. The auto engineered features would also need to be computed for each new value - if anyone is interested in a recipe step for featuretools please tweet me @lopp_sean.\nOther Things to Try\nI didn’t spend much time on modelling here, it could be interesting to try more models and to try fitting with cross validation instead of just a single training / testing split. To do cross validation here we would need to watch for time, the timetk package has a new function to help with this: timetk::time_series_cv.\nIf you don’t think we did enough feature generation there is one other avenue to try, which is lagging or rolling the episode rating. It is possible show ratings carry “momentum” that could be predictive of the next show’s rating.\n\n\n",
    "preview": "posts/tasty-models/distill-preview.png",
    "last_modified": "2024-04-23T20:22:42-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-11-03-spark-a-r-python-aphrodisiac-or-ender/",
    "title": "Spark?",
    "description": "If R & Python is a love story, is Spark an aphrodisiac or a relationship red flag?",
    "author": [
      {
        "name": "Sean Lopp",
        "url": {}
      }
    ],
    "date": "2020-11-03",
    "categories": [
      "data_science"
    ],
    "contents": "\nNot very original memeI’ve written about how R and Python can be used together:\nin a platform\nby package authors writing multilingual wrappers\nby individuals in an analysis\nRecently I was presented with a challenge that was well suited to Spark and forced me to think about the R and Python “love story” at scale. Or in other words, could R and Python be used together in a ML engineering context that values performance and scale?\nIf you are fuzzy on the differences between analyst and ML engineer, I highly recommend Cassie Kozyrkov’s breakdown1. My goal in this post is to show you some tricks that can help a multi-lingual analyst approach an ML engineering task by taking advantage of both Python and R.\nThe Setup\nImagine you’ve got customer data in a time series:\n\n\n# A tibble: 11 x 4\n   customer time                water_flow customer_type\n   <chr>    <dttm>                   <dbl> <chr>        \n 1 cust_A   2019-01-15 08:00:00       6.25 business     \n 2 cust_A   2019-01-15 08:15:00      18.8  business     \n 3 cust_A   2019-01-15 08:30:00       1.75 business     \n 4 cust_A   2019-01-15 08:45:00       3.5  business     \n 5 cust_A   2019-01-15 09:00:00      20    business     \n 6 cust_B   2019-03-20 11:30:00      13    residential  \n 7 cust_B   2019-03-20 11:45:00      13    residential  \n 8 cust_B   2019-03-20 12:00:00       8    residential  \n 9 cust_B   2019-03-20 12:15:00      17.8  residential  \n10 cust_B   2019-03-20 12:30:00       6.25 residential  \n11 cust_C   2019-10-11 22:45:00      15.5  business     \n\nNow imagine, additionally, you’ve done some feature engineering and built a model. In this case, say our model is attempting to predict whether or not a customer is a business or a residential water user.\nOn many data science teams, you may have Python code that looks something like this:\n\n\nimport pandas as pd\nfrom scipy.signal import find_peaks\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\n\ntraining_data = pd.read_csv(\"water_data.csv\")\nnum_customers = len(training_data.customer.unique())\n\n# take the time series and generate\n# one feature row per customer\nnum_peaks = np.zeros(num_customers)\nmax_water_off_hours = np.zeros(num_customers)\nmax_water_abs = np.zeros(num_customers)\n\nfor c in 1:num_customers:\n  customer = training_data.customer.unique()[c]\n  this_customer = training_data[training_data.customer == customer]\n  # use a cool scipy algorithm\n  peaks, _ = find_peaks(this_customer.water_flow)\n  num_peaks[c] = len(peaks)\n  # some simpler feature generation\n  hour = this_customer.time.hour\n  max_water_off_hours[c] = max(this_customer.water_flow[hour < 8 | hour > 17])\n  max_water_abs[c] = max(this_customer.water_flow)\n  \nX = pd.DataFrame.from_dict({\n  'num_peaks': num_peaks,\n  'max_water_off_hours': max_water_off_hours,\n  'max_water_abs': max_water_abs\n})\n\ny = (training_data.groupby(\"customer\")\n  .customer_type\n  .agg(lambda column: pd.unique(column)))\n\nmodel = LogisticRegression().fit(X, y)\n\nSo here is the big setup: What if you need to score 1 million customers? And what if your expertise is in R?\nOne way to frame the second condition of the questions is what if, as an analyst, you are comfortable with some Python and some R but not all that comfortable with PySpark? Have no fear, this blog post has the tips for you!\nWhy Spark?\nSpark is a natural fit for this example (and other problems like it) for a few reasons:\nThe data really is BIG. In this case, if each customer is a time series with data every 15 minutes, in a year a customer would generate 35K rows of data. 1M customers would generate close to 35 billion rows of data in a year. With data this size, feature generation in a loop would be tricky.\nThe data is well suited to partitioning. In this case, each customer is still relatively small, and assuming the customers are independent, you have a very natural map reduce problem. In laymen terms, this means the problem is easy to break into small pieces. We could write parallel code ourselves, but if you have big data that is easy to break into small pieces, than the Spark ✨ is really just that it will do the parallelization for you!\nThe Love Story Challenge\nUnfortunately the average data analyst, asked to translate the code above into Spark, could be stumped. And this is where I will make a bold statement. In this case, combining R and Python with a few tricks lets you accomplish the ML engineering task.\nTips and Approach\nStep 1: For local testing, get Spark installed and running.\nFor this step, I find the R toolchain to be really well suited to the task:\n\n\nlibrary(sparklyr)\nspark_install()\n\nStep 2: Feature Transforming using dplyr\nAs a bilingual analyst, this is where we can start to cash in on some sparklyr magic. Even though the original feature transformation code is in Python, we can use dplyr in R to quickly generate the Spark SQL equivalent:\n\n\n# connect to our dev spark environment\nsc <- spark_connect(master = \"local\")\n\n# take our Python for loop and turn it into a \n# dplyr chain\ntraining_data <- spark_read_csv(\"water_data.csv\")\n\nfeatures <- training_data %>% \n  group_by(customer) %>% \n  mutate(hours = to_hour(time)) %>% \n  mutate(\n    off_hours = as.numeric(hours < 8 | hours > 17),\n    max_water_off_hours = max(water_flow * off_hours), \n    max_water_abs = max(water_flow)\n  )\n\ndbplyr::sql_render(features)\n\n\nSELECT `customer`, `time`, `water_flow`, `customer_type`, `hours`, `off_hours`, MAX(`water_flow` * `off_hours`) OVER (PARTITION BY `customer`) AS `max_water_off_hours`, MAX(`water_flow`) OVER (PARTITION BY `customer`) AS `max_water_abs`\nFROM (SELECT `customer`, `time`, `water_flow`, `customer_type`, `hours`, CAST(`hours` < 8.0 OR `hours` > 17.0 AS DOUBLE) AS `off_hours`\nFROM (SELECT `customer`, `time`, `water_flow`, `customer_type`, to_hour(`time`) AS `hours`\nFROM `training_data`) `dbplyr_001`) `dbplyr_002`\nThe first key trick: Sometimes when you are writing dplyr that executes against a Spark data frame, you want to use native R functions, especially in your mutate. Sometimes these R functions work magically, sometimes they do not. In general, you can see what R functions will magically make the trip to Spark SQL here. If your function isn’t on the list, fear not. Often there is a Hive UDF that will do the trick. Think of a Hive UDF as a special function you can use in spark-flavored dplyr. The list of these functions lives here, and in this example we use to_hour. \nThe second key trick: You can now take this SQL and use it in a different context. In our example, we will need to use a Python UDF (see below) to capture the find_peaks feature transformation. This means our final Spark instructions will need to run in Python. But we don’t need to learn all of PySpark, we can take our dplyr-generated Spark SQL and use in Python like so:\n\n\nsc = (SparkSession\n  .builder\n  .appName(\"DetermineWater\")\n  .getOrCreate())\n\n\n# read our data directly into spark\ntraining_data = (sc.read.options(header=True, inferSchema=True, delimiter=\",\")\n  .csv(\"water_data.csv\"))\n\n# take our spark data frame and create a view that we can execute SQL against\ntraining_data.createOrReplaceTempView(\"training_data\")\n\n# embed our direct-from-dplyr SQL\n# WOW isn't it still nice and readable :) \nfeature_query = \"\"\"\nSELECT `customer`, `time`, `water_flow`, `customer_type`, `hours`, `off_hours`, MAX(`water_flow` * `off_hours`) OVER (PARTITION BY `customer`) AS `max_water_off_hours`, MAX(`water_flow`) OVER (PARTITION BY `customer`) AS `max_water_abs`\nFROM (SELECT `customer`, `time`, `water_flow`, `customer_type`, `hours`, CAST(`hours` < 8.0 OR `hours` > 17.0 AS DOUBLE) AS `off_hours`\nFROM (SELECT `customer`, `time`, `water_flow`, `customer_type`, to_hour(`time`) AS `hours`\nFROM `training_data`) `dbplyr_001`) `dbplyr_002`\n\"\"\"\n\nfeatures = sc.sql(feature_query)\n\nStep 3: Python UDFs\nNow that we have the features we can generate from SQL (thanks to our dplyr trick), it is time to turn to the peaks feature that comes from the scipy find_peaks function. We could approach the peaks transformation at scale in a few ways:\nWe could try to find a built-in Spark equivalent to find_peaks\nWe could try to write a SQL equivalent ourselves\nWe could use a “user defined function” to execute the Python code in Spark\nOption 3 is a good choice here thanks to the recent work by the Arrow Team. There is now the option in Spark to efficiently run a Python function that takes in a data frame partition (a small piece of our BIG data) and returns a data frame, and it is all done as-if the input and output were pandas data frames. Here is what the code looks like:\n\n\n# a regular python function that takes a pandas dataframe and \n# returns a pandas dataframe\ndef peak_finder(this_customer):\n  peaks, _ = find_peaks(this_customer.water_flow)\n  return(\n   pd.DataFrame.from_dict({\n    'customer': this_customer.customer[0]\n    'num_peaks': len(peaks)\n   })\n  )\n  \n# this is the only weird part, we have to give spark\n# more info about the data frame we'll be returning\nschema = \"customer string, num_peaks int\"\n\n# call the magic, Spark dispatches the Python function\n# taking advantage of the per customer partition \npeaks = (training_data\n  .groupby(\"customer\")\n  .applyInPandas(peak_finder, schema = schema))\n\nWith peaks and features both at hand, we could create a final features Spark data frame using join.\nFinal Step : Model (Re)Fitting and Scoring\nA benefit of Spark is that it provides some ML capabilities out of the box that the analyst might already be familiar with. In this hypothetical case we could do two things:\nIf we were happy with our trained model, we could just apply the coefficients the old fashioned way to our BIG data in Spark by writing code that looks like y = coef*X1 + coef*X2 … . (Or for a logistic regression, \\(y = \\frac{e^{X\\beta}}{1 + e^{X\\beta}}\\) ).\nIf we wanted to train a new model, we could take advantage of Spark’s own logistic regression, which is purpose built for fitting model coefficients in the case where the total data (1M customers minus a holdout) is too big for your local machine.\nNow that we have the pseudo code to make our concept work in Spark, scaling is as easy as: (1) reading in a folder of csvs instead of a single csv, (2) connecting to a real Spark instance instead of our local development instance, and then (3) scaling the cluster horizontally and tuning our spark commands to take advantage! Ok, I know, this last sentence may feel a bit like “draw the owl”, but a blog post can only contain so many 💎.\nConclusion\nMultilingual data science can make you a ninja. Two tricks in particular are really powerful: dbplyr::sql_render and applyInPandas. Combining these tactics from different languages allows the regular data scientist to achieve ML engineering status: performance at scale. In this case, I think it is fair to say that R and Python are compatible with Spark, and I might go so far as to say Spark is an aphrodisiac to the multilingual love story.\nGoogle’s Chief Decision Scientist. Essentially, her point is that analysts value speed, statisticians value rigor and correctness, and ML engineers value performance. So in this case, my question is whether R and Python can be used together in a performant and scaled way, not just to enable speedy data exploration and hypothesis discovery.↩︎\n",
    "preview": "posts/2020-11-03-spark-a-r-python-aphrodisiac-or-ender/images/paste-9BBAFB7B.png",
    "last_modified": "2024-04-23T20:22:42-06:00",
    "input_file": {},
    "preview_width": 750,
    "preview_height": 500
  },
  {
    "path": "posts/2020-10-27-a-windy-tidytuesday/",
    "title": "A Windy TidyTuesday",
    "description": "Using crosstalk to analyze spatio-temporal data.",
    "author": [
      {
        "name": "Sean Lopp",
        "url": {}
      }
    ],
    "date": "2020-10-27",
    "categories": [
      "data_science"
    ],
    "contents": "\nProbably Not Canadian Windmills\n\nIt has been a long time since I’ve done a TidyTuesdsay post, but wind turbines, time series, and maps are hard to resist! My goal for this dataset was to make a standalone webpage housing an interactive dashboard by using crosstalk.\nYou can learn a bit about this week’s dataset here, but essentially we have spatio-temporal information on Canadian wind turbines. My overall goal was to present an option for looking at the data across time, but immediately I discovered the “time” component of our dataset - commissioning_date - needed a bit of clean up. Who has a column where some values are 2001 and some are 2001/2002? 😞 Unfortunately there didn’t appear to be an easy way to fully determine the actual commission date. A couple of options presented themselves:\nI could just “floor” the project date. So a project with a commission date like 2001/2002 would represent all the turbines with year = 2001 . This option is the easiest but the least satisfying.\nI could create some kind of heuristic to assign turbines to years assuming a linear build out. For example, if a project had ten turbines split across 2 years (e.g., 2001/2002) I would assign the first 5 to 2001 and 6-10 to 2002.\nSince I was on vacation, I decided to go with option 2.\n\n\n# compute year based on turbine's rank \n# in project and available commissioning years\n# assuming a linear roll out over time\ncompute_year <- function(turbines){\n  # given data like 2001 / 2002\n  num_years = str_count(turbines$commissioning_date, \"/\")\n  cut_points = turbines$total_turbines / (num_years + 1)\n  # ceiling here because we want to start \n  # our counting at year1 (see separate call below)\n  turbine_yearid = (ceiling(turbines$turbine_rank / cut_points)) %>% \n    sprintf(\"year%d\", .)\n  turbine_year = map_chr(\n    1:nrow(turbines), \n    ~turbines[[.x,turbine_yearid[.x]]]\n  )\n  turbine_year\n}\n\n# pre-process \nwind<- wind_turbine %>% \n  # split the gross date field\n  # ... with grosser code hard coded to assume\n  # at most 3 annual values :grimmace: \n  separate(commissioning_date, \n           into = c(\"year1\", \"year2\", \"year3\"), \n           sep = \"/\", \n           remove = FALSE) %>% \n  separate(turbine_number_in_project, \n           into = c(\"turbine_rank\", \"total_turbines\"), \n           sep = \"/\", \n           remove = FALSE) %>% \n  mutate(turbine_rank = as.numeric(turbine_rank),\n         total_turbines = as.numeric(total_turbines)) %>%  \n  # call our function\n  mutate(computed_year = compute_year(.)) %>% \n  mutate(computed_year = paste0(computed_year,\"-01-01\"),\n         computed_year = ymd(computed_year),\n         year = year(computed_year)) %>%  # needed later...\n  # prettier labels\n  mutate(label = paste0(\n    project_name, \" <br/>\", \n    total_project_capacity_mw, \" project MW <br/>\",\n    turbine_rated_capacity_k_w, \" turbine kW <br/>\"\n  ))\n\n# look at an interesting case to see what all this code did\nwind %>% \n  filter(project_name == \"St. Leon\") %>% \n  select(turbine_identifier, \n         turbine_number_in_project, \n         commissioning_date, \n         computed_year, \n         starts_with(\"year\"), \n         -year) %>% \n  rmarkdown::paged_table()\n\n\n\nBefore we go any further, we should check if our “computed year” heuristic makes any sense. To do this, we’ll pick an interesting project (one split over multiple commissioning years) and then check if the breakdown makes sense.\n\n\ncolors <- c('darkred','purple', 'orange')\nst_leon <- wind %>% \n  filter(project_name == \"St. Leon\") %>% \n  # mutate for fixed circle sizes below\n  mutate(turbine_capacity_10kw = turbine_rated_capacity_k_w / 10, \n         color = colors[as_factor(year)]) %>% \n  SharedData$new()\nyear_select <- filter_checkbox(\n  id = 'year',\n  label = \"Year\",\n  sharedData = st_leon,\n  group = ~computed_year\n)\nmap <- leaflet(st_leon) %>% \n  addProviderTiles(providers$CartoDB.Positron) %>% \n  # use a addCircles to get a fixed size \n  #since we don't expect much zooming on this plot\n  addCircles(lat = ~latitude, \n             lng = ~longitude, \n             radius = ~turbine_capacity_10kw, \n             popup = ~label,\n             color = ~color)\n\n# lay out widgets\ndiv(\n  bscols(\n    # in bscols things add to 12!\n    widths = c(2, 10),\n    year_select, \n    map\n  )\n)\n\n\n\n\n\nYear\n\n\n\n2005-01-01\n\n\n\n\n2006-01-01\n\n\n\n\n2012-01-01\n\n\n\n\n\n\n\n\n\n\n\n\nYou can see how our “computed year” huerisitic mostly corresponds with geographic area. Each year’s build out was in a new area, which makes sense to me because that is how construction works. This gives me some reassurance that the heuristic is reasonable.\nNow that we have some confidence our data is correct, we can use crosstalk to power an exploration.\n\n\n# create the object for crosstalk\nwind_sd <- SharedData$new(wind)\n\n# for some reason distill \n# and date driven sliders\n# dont get along\nyear_slider <- filter_slider(\n  id = \"year\",\n  label = \"Year\",\n  sharedData = wind_sd,\n  column = ~year,\n  sep = \"\"\n)\n\nprovince_filter <- filter_select(\n  id = \"prov\",\n  label = \"Focus on one province\",\n  sharedData = wind_sd,\n  group = ~province_territory\n)\n\nmap <- leaflet(wind_sd) %>% \n  addProviderTiles(providers$CartoDB.Positron) %>% \n  # use circle markers so that the mark scales as you zoom\n  addCircleMarkers(lat = ~latitude, \n             lng = ~longitude, \n             popup = ~label\n  )\n\npower_plot <- plot_ly(wind_sd,\n  y=~province_territory,\n  x=~turbine_rated_capacity_k_w,\n  type = \"bar\",\n  transforms = list(\n    list(\n      type = 'aggregate',\n      groups = ~province_territory,\n      aggregations = list(\n        list(\n          target = 'x', func = 'sum', enabled = T\n        )\n      )\n    )\n  )\n) %>% \nlayout(\n  title = 'Total Available Power (kW)',\n  yaxis = list(title = 'Province'),\n  xaxis = list(title = 'Total Rated Capacity kW')\n)   \n\nheight_plot <- plot_ly(wind_sd,\n  x=~province_territory,\n  y=~hub_height_m,\n  jitter = 0.7,\n  type = \"scatter\",\n  color = ~year\n) %>% \nlayout(\n  title = 'Turbine Size',\n  xaxis = list(title = 'Province'),\n  yaxis = list(title = 'Hub Height (m)'),\n  color = list(title = 'Year Turbine Comissioned')\n)\n\n# lay out widgets\ndiv(\n  bscols(\n    # bscols total is 12\n    widths = c(2,5,5),\n    # uses lists to represent \"rows\" within a column\n    list(year_slider, province_filter),\n    power_plot, \n    height_plot\n  ),\n  map\n)\n\n\n\n\n\nYear\n\n\nFocus on one province\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne final note. As you can see in the map, there is a bit of overplotting… One way to handle this normally in leaflet is to use clusterIds, which I demonstrate below. Unfortunately at this time clustering is not supported with crosstalk filters.\n\n\nleaflet(wind) %>% \n  addProviderTiles(providers$CartoDB.Positron) %>% \n  # use circle markers so that the mark scales as you zoom\n  addCircleMarkers(lat = ~latitude, \n             lng = ~longitude, \n             popup = ~label, \n             radius = ~turbine_rated_capacity_k_w/100,\n             clusterId = ~project_name,\n             clusterOptions = markerClusterOptions()\n  )\n\n\n\n\n",
    "preview": "posts/2020-10-27-a-windy-tidytuesday/images/paste-A30A7B7C.png",
    "last_modified": "2024-04-23T20:22:42-06:00",
    "input_file": {},
    "preview_width": 510,
    "preview_height": 340
  },
  {
    "path": "posts/2020-10-26-tidymodels-dvc-mashup/",
    "title": "Tidymodels DVC Mashup",
    "description": "Using Github Actions and Data Version Control for ModelOps in R",
    "author": [
      {
        "name": "Sean Lopp",
        "url": {}
      }
    ],
    "date": "2020-10-26",
    "categories": [
      "data_science"
    ],
    "contents": "\nI had a few goals in my first few days of vacation:\nFigure out GitHub Actions.\nTry out tidymodels , after being inspired by a R in Pharma talk on the stacks package.\nInvestigate this whole MLOps thing, and by investigate I meant try it, I did NOT go on vacation to read more Gartner Garbage ™️ .\nAs luck would have it, @DrElleOBrien happened to reach out about trying DVC with R. DVC is a framework for MLOps that can use GitHub actions, so I had the perfect excuse to knock out all three goals with one sample project.\nIf you don’t want the rambling story, here is the project: https://github.com/slopp/tidydvc.\nBackground\nDVC\nDVC, and its close companion CML, provide tools for model management - think Git for data and models. The core idea is that a DVC pipeline tracks the input (files, data, hyper-parameters) to an experiment and the output of an experiment (model, metrics, etc). DVC can be used locally, but the real magic is that DVC can be combined with something like GitHub and GitHub actions to automate the experiment management. Just like a software engineer could create a PR to propose a change to code, a data scientist could create a PR to propose a change to a model. But, unlike in software engineering where the goal of a PR is to review and automatically test code changes, in ModelOps the goal of the PR would be to train a model and explain the outcome!\nDVC is primarily built around Python, but I wanted to see if it could be used with R. In many ways it shares some of the principles of the drake (now targets) package, but with some added Git ✨.\nTidymodels\nTidymodels is an opinionated workflow for model construction in R1. It is kind of like scikit learn in the sense that many different algorithms are presented with a consistent interface.\nA few interesting bits about tidymodels:\nThere is a concept of a workflow that allows you to track a model’s pre-processing steps and the model itself in one easy-to-use object. This turns out to be super helpful, though a bit at odds with DVC (or drake)’s concept of a pipeline… more to come.\nThere are easy ways to tune hyper-parameters. Again, very helpful, but potentially at odds with external ModelOps-y things that want to control those elements.\nThere is experimental support for ensembles, which is what I wanted to try!\n\nMashup\nAside from learning the individual pieces, I wanted to see if it was possible to glue these different ecosystems together. As I hinted at above, however, there were some surface level concerns. It wasn’t clear to me if I could define clean boundaries between the concepts tidymodels wanted to control and what DVC expected to own. But the goal of PRs with beautiful ggplotswas enticing. (Famous last words).\nFinal note, I also enjoy using R Markdown as a framework for data exploration and model creation. I find R Markdown encourages “thinking while coding”. I definitely wanted R Markdown to play a role.\nIf you are interested in this type of mashup but want to play mostly in R, I highly recommend this article by David Neuzerling: Machine Learning Pipelines with Tidymodels and Targets.\nUnstacking Stacks\nI began by creating an R Markdown document that roughly followed the outline of this stacks tutorial. To spice things up, I decided to use the Ames Housing data, so my goal was to create an ensemble that predicted housing prices. You can follow the code here. Because one of my goals was to try tidymodels, here are some notes for future me on what I learned.\nDon’t skip EDA\nI didn’t want to just copy code from Max’s parsnip regression tutorial so instead I opted to write my own formula regressing on different predictors2. I thought I could do this without any EDA. MISTAKE. I lost at least 2 hours fighting models that wouldn’t converge because my data was either incomplete or my pre-processing was non-sensical. I still am not happy with the results, but I did learn a valuable tactic. When defining a recipe that will be used further down the line in a workflow, it is still a good idea (though not required) to prep and juice the recipe. Essentially, this optional juicing allows you to SEE how the recipe is applied to your training dataset and troubleshoot any errors in a friendly space, as opposed to an unfriendly space (which is what I call a stack trace from the middle of a cross validation optimization routine that has failed because of a NA).\n\n\n# First define the recipe\nhousing_rec <- recipe(SalePrice ~ ...) %>% \n  update_role(PID, new_role = \"ID\") %>% \n  step_dummy(all_nominal()) %>%  \n  step_meanimpute(all_numeric()) %>% \n  step_zv(all_predictors()) %>% \n  step_nzv(all_predictors())\n\n# Check it!\nhousing_rec %>% \n  prep() %>% \n  juice()\n\n# Then do the rest of your workflow magic\nhousing_wflow <- \n  workflow() %>% \n  add_recipe(housing_rec)\n\nTune\nTuning is easy. Getting the model out of the tune to actually use… a little less intuitive. As with all things, I should have started by reading Julia Silge’s advice. Essentially there are a couple things you (future me) need to know:\nYou create a model spec and stick it in a workflow:\n\n\n# PS - for those who learned penalized regression through glmnet\n# penalty = lambda \n# mixture = alpha\nmodel_spec <- linear_reg(penalty = tune(\"penalty\"), mixture = tune(\"mixture\"))\n\n# the workflow ties your model to your pre-processing\nmodel_wflow <- \n  workflow() %>% \n  add_recipe(preprocess_data) %>% \n  add_model(model_spec)\n\nYou can tune the model3 using a grid:\n\n\ntune_results <- \n  tune_grid(\n    model_wflow,\n    resamples = folds,  \n    metrics = metric,  # think rmse\n    control = control_grid() # controls what metadata is saved \n  )\n\nYou can look at the tuned results:\n\n\n# hint use the n function argument to see more. This is where you can find the actual tune parameters\nshow_best(tune_results)\n\nBUT, at this point, you have to finalize the actual best model. Or in the case of an ensemble, you have to finalize many models. Finalize means take the tuned parameters and fit the model on ALL the training data. (The parameters come from a model fit on a cross validation fold). To do this:\n\n\n# select_best was counterintuitive to me... I found it hard to pull the data\n# for all models; and it doesn't give you the best model, just the best\n# tune parameters \ntuned_params <- select_best(tune_results)\n\n# use this object for future predictions\nmodel_wflow_fit <- finalize_workflow(model_wflow, tuned_params)\n\n# final tip: to inspect the actual model \nmodel_wflow_fit %>% \n  pull_workflow_fit() %>% \n  broom::tidy() # linear models OR\n  vip::vip() # trees\n\nAt different points of this process it can be easy to forget what is trained and what is still a placeholder. The print methods are really good, use them.\nStacks\nMy final goal was to fit an ensemble model. Unfortunately, I hit an error using the stacks package. I suspect this error was due to the fact that the package is experimental and I had some stale dependencies. This left me between a rock and, well, another rock. I could have tried to nuke my environment and fight with remotes to install a bunch of development tidymodels packages. (This was made harder by the WIP change to main from master 👏). Or I could try to push forward by working around the error with my own ensemble code. I went for the latter path, and refreshed a lot of my statistical thinking along the way. (Recall %*% is how to do matrix multiplication in R). But I also killed 5 hours of vacation. 🤷 I will spare you and my future self the details in the hope that the error is fixed, but a few conceptual tips:\nThe stacks() object starts with candidate models (from tune_grid) or a single model but with multiple fits (from fit_resamples). The magic this function does is align the different models based on the resample they used. In other words, if one candidate was fit on resample 1, it is matched with another candidate fit on resample 1. The same matching occurs within tuning configurations from a single model.\nUnder the hood, the ensemble is fit with glmnet, but buried in quite a few layers of tidymodels abstraction. cv.glmnet works as an alternative, but with a nasty side affect. You have to do all the work manually to get a final fitted object that glues all the prior models, workflows, and the ensemble together.\nWhen those nice print methods I mentioned earlier fail, try attributes to see “all the stuff” tidymodels is carrying around.\nThe Mashup\nAdding DVC\n\nOnce I had a functional R Markdown document that was fitting models, it was time to try this ModelOps thing. Now ModelOps can consist of a lot of things, but in this case I was interested in trying out the training side of model operations - tracking experiments and enabling collaboration. I’ll save the monitoring side - deploying models to production and watching them overtime - for another vacation.\nDVC is oriented around pipelines. Unfortunately, this appeared to clash a bit with my R Markdown workflow. For one, the R Markdown document “drove” all the modeling code, and this code was not amenable to being managed by an external pipeline. Another problem was that the tidymodels packages, especially tune and stacks, handled the parameters and “experiments” that DVC wanted to control.\nI decided to mostly ignore the first challenge. If my data was large, or the pre-processing intense, I think I would have benefited from breaking the R Markdown document into child documents that could each be a stage in a pipeline. In this case, I didn’t care for the extra overhead in saving intermediate results, and things were fast enough that I didn’t mind re-running steps from scratch. As I mentioned, I am advocate for using R Markdown instead of .R files even in the multi-stage approach. You an see an example of this “split it up” approach here.\nFor the second problem, I decided to go with a hyper-hyper-parameter strategy. Essentially, I allowed tune and tidymodels to handle the hyper-parameter optimization within a model, but I used DVC to handle “parameters” that would fundamentally change the type of model at play. In other words, my “PR experiments” amounted to asking questions like “what if our ensemble uses ridge regression instead of LASSO?” as opposed to questions like “what if each forest has more or less trees?”. Is this splitting hairs? You betcha. In fact, you can see artifacts in the code where I did use DVC for “# of trees” and where I attempted to use the tidymodels code for optimizng the ensemble. My advice: “experiments” in most ModelOps settings come with overhead and complexity, so use them to track whatever changes really matter to you.\n\nIn the current state of ModelOps, there aren’t great boundaries.\n\nI think this article explains this nascent border-less state well.\nOne artifact of my toy exploration is that I also punted a bit on questions of scale. One of the claims of ModelOps is that, when adopted correctly, the training portion of experiments can easily be scaled across a wide variety of backends. I’ll leave that test for another day, but in my experience “magic parallelism” is almost always a lie.\nPipeline Iteration\nPipeline FailuresAfter settling on how I wanted to incorporate DVC into my example, it was time to try and get GitHub Actions working. This is where I thought my expertise in R package management would come in handy. I was right and wrong. On the plus side, I was successful. On the other hand, it was still hard.\nHere are a few tips and tricks:\nElle created an action workflow that showed me how to use the r-lib Github actions to install R in the context of a CML pipeline. Unfortunately, the r-lib actions are geared towards people testing packages, not people just running code. This discrepancy made our use case a bit awkward. In the future there will hopefully be actions geared at using R to simply run code.\nI created an R script to wrap my render::rmarkdown call. This script did two things. First, it set the R repo option to use Public Package Manager to provide pre-built package binaries. This rapidly speeds up package install time. Second, the script calls renv::restore, which installs the packages I needed for my project. renv is like the R equivalent to pip freeze; pip install -r requirements.txt .\nEven though I was using pre-compiled R package binaries, some of them still had runtime system requirements. This meant I had to add a number of apt-get install thing lines to the actions file. I should have known this ahead of time and used Package Manager to identify what those requirements would be, but instead I did it the “old fashioned” (read: painful) way of failing again and again until it worked.\nOverall, I think if I was doing a project like this for real, it’d be well worth my time to create a custom Docker image with my project environment and have the GitHub Action start by pulling that image. Otherwise you end up spending valuable iteration time waiting for the same packages to be installed.\nI also think this experience highlights the importance of local testing and experimentation. Running dvc repro to try out experiments locally was significantly faster for both troubleshooting and model exploration than waiting for the CI system to run. It is also much easier, locally, to explore metrics around model fit or interpretation that you may have forgotten to have your CI system write out. What this means:\nBefore CI/CD can be real for ModelOps, CI systems will need to be optimized for model operation performance.\nModel management tools, like DVC pipelines, can be useful for organizing experiments and collaborating with others even without the CI part.\nIf you do go the CI route, remember that the environment is ephemeral, so be sure to save persistently everything and anything that might be useful for interactive debugging, model interpretation, etc later on.\nIn general, I think we are a ways away from collaborating as seamlessly as software engineers do through Git. But, the state of the world is much much better than tracking model runs manually in a spreadsheet!\nThe Magic Moment\n\n“There is nothing quite as joyous as the rush one gets from a pipeline building when it had been failing” -every software engineer and soon every data scientist too\n\nThis PR shows you the results, and here are a few of the happy screenshots:\nMetric DiffsDVC Pipeline with RGitHub PRs for ModelOpsTip: it is https://tidymodels.org NOT https://tidymodels.com…↩︎\nMy ultimate model did outperform the models in that tutorial …. but lets all admit tutorial models are garbage and safely move on. Lest we get stuck on wondering why folks in Iowa value garage spaces over lot size.↩︎\nI am going to use model and workflow as synonyms for the rest of this section↩︎\n",
    "preview": "posts/2020-10-26-tidymodels-dvc-mashup/images/modelops.png",
    "last_modified": "2024-04-23T20:22:42-06:00",
    "input_file": {},
    "preview_width": 1142,
    "preview_height": 693
  },
  {
    "path": "posts/2020-10-18-hamburgers-at-chick-fil-a/",
    "title": "Hamburgers at Chick-fil-a",
    "description": "An inside look at RStudio's decision to embrace Python",
    "author": [
      {
        "name": "Sean Lopp",
        "url": {}
      }
    ],
    "date": "2020-10-18",
    "categories": [
      "product"
    ],
    "contents": "\nStrategic Hamburger?In the fall of 2018, if you were to guess who was advocating for Python at RStudio, you may have guessed it was me. I had given a webinar on using R and Python together with the brilliant reticulate package. I had written about Python powered shiny applications. As a technologist I was excited. It was fun to glue the two languages together, the details of the C++ implementation were intriguing, and the results, in true JJ fashion, were magical.\nBut all that magic aside, as a product manager, I was surprised when both JJ and Tareef proposed adding support for Python across RStudio Team. My role at RStudio was to shepherd the professional products (RStudio Team) that help fund RStudio’s open source work. Most commonly, this role meant balancing the grand visions of JJ and Tareef with the immediate requests of our customers, engineers, and support teams. In this case, the grand vision came first. Tareef and JJ were concerned that data science teams wouldn’t purchase tools just for R. They wanted to ensure RStudio Team could stand against other multi-lingual platforms. I was afraid we would loose our competitive advantage. The strategy for RStudio’s professional products is to build incredible experiences for users, based on the open source foundations we own and control. To me, the idea of adding Python into the fabric of RStudio Team was akin to Chick-fil-a adding hamburgers to their menu. Would the brand crumble? Would the tools miss the mark? Would users revolt?\nWe began by talking to customers and trying to identify a first step. Jupyter Notebooks were the lingua franca, despite a user experience that, in my view, was pretty lack luster1. We soon found a pretty interesting anecdotal trend. Contrary to the intuition I had gathered from a weak exposure to Python twitter, it turned out Python data scientists faced a similar challenge to R users: while they were writing code they were not software engineers. This meant that, just like R users, they struggled with deployment and sharing their work. It turned out Python data scientists found docker just as intimidating as R users2. Even larger organizations running JupyterHub weren’t thrilled with the experience of sending executives links to the clunky Jupyter interface.\nWe hatched a plan to add support for Jupyter Notebooks to RStudio Connect, our platform for sharing data products. The execution model would be similar to R Markdown, Notebooks could be re-rendered on a schedule, but the default experience for users would be a pre-rendered HTML page without an active kernel. Vice president proof. We also wanted to make publishing just as easy as it was in R, enabling a single button push to move a notebook form local development to production3. This desire meant quite a bit of plumbing, figuring out how to capture a user’s python dependencies, and how to recreate the right environment on the server (for scheduled re-runs).\nPublishing to Connect from JupyterThe reception to the change in Connect was slow but positive. Users didn’t revolt. The brand didn’t crumble. Pretty soon we were logging and tracking requests for other types of content. Data science teams familiar with plumber APIs and Shiny saw opportunities for their Python counterparts to deploy something interactive without talking to IT.\nBut even given this success, I was still skeptical. We wanted to add support for Jupyter to RStudio Server Pro, turning the professional IDE into an IDE workbench. I hated the idea. There were also plans to add PyPI mirroring to RStudio Package Manager, a move that would be expensive and face stiff competition.\nFast forward two years and the bets have made progress, the results are promising but pending. We haven’t lost any business because we’re R-only. (Though who knows what the counter factual would entail). The analysts eat up marketing’s “love story”. Our data from customers suggest that Jupyter in both RStudio Server Pro and Connect is being used, and adoption of other Python content types will likely follow as customers upgrade versions.\nBut as a product manager I still wonder if we’ll be able to build a truly differentiated user experience without going deeper into the Python open source ecosystem. Typically product management is responsible for ensuring the right thing is built for the right reasons. At RStudio, however, most of the magic has come from open source engineers sitting in the trenches with users. We’ll have to build that muscle for Python. I see promise as the RStudio IDE adds richer Python support and I wonder if the improvements to R Markdown editing will be just as game changing to Python Markdown editing.\nSo what is the lesson here? I think the morale to this rambling story is simple: product management is hard. At one time you have to sell a vision to engineers and in-progress products to customers, all while being in the best position of anyone to have doubts. You see the opportunity cost of every bet - why take this risky big move when there are small sure bets? You see the cracks in the data, the counter arguments to decisions. My advice if you find yourself in these situations is to stretch the time horizon. RStudio’s goal is to build a durable product, measured in decades not VC rounds. When you stretch the time horizon you allow yourself to watch bets play out, to adjust as data comes in, and to evolve as the market adjusts.\nIn the grand scheme, I am also proud of a change in the market that we’ve witnessed. Two years ago the R vs Python debate raged. Now, in a world where mutually exclusive polarization is the norm, I see more teams talk about using R and Python. I see more job descriptions open to either. I don’t know if our “love story” and product investment moved the needle. I hope it did, and I hope it continues to. Regardless of the bets we make in our commercial products, I hope to see data scientists encouraged to use tools they love in any language.\nR and PythonWhen you are used to using R Markdown in a full fledged IDE, using a tool where you have to insert a chunk to write markdown feels very clunky.↩︎\nI think many make a mistake by not segmenting Python into Python engineers and Python data scientists.↩︎\nAt some point on this blog I’ll discuss why we decided to use pip freeze for this support instead of conda.↩︎\n",
    "preview": "posts/2020-10-18-hamburgers-at-chick-fil-a/images/paste-4AF33372.png",
    "last_modified": "2024-04-23T20:22:42-06:00",
    "input_file": {},
    "preview_width": 1200,
    "preview_height": 934
  },
  {
    "path": "posts/2020-10-26-parks-and-rec-management-tips/",
    "title": "Parks and Rec",
    "description": "I learned my most useful management skill from Ann Perkins",
    "author": [
      {
        "name": "Sean Lopp",
        "url": {}
      }
    ],
    "date": "2020-08-12",
    "categories": [
      "management",
      "product"
    ],
    "contents": "\nWine and CheeseParks and Rec is one of my all time favorite shows, behind Michael Schur’s other hit, The Good Place, which is a masterpiece. While it lacks the magic realism, Parks and Rec does have the rich and relatable characters Schur is known for, and I’ve found they can teach quite a few work place lessons.\nFor example, this scene has some of the best product management advice I’ve ever heard:\n\nAndy: I can’t go back to London, I’m in totally over my head on this projedt. Every day someone comes up to me and says I need your approval on this Mr. Dwyer, I need your signature Mr. Dwyer…\nApril: But you said everything was going awesome… [Chuck Norris references]\nAndy: … and is scared and confused about his big London job…\nApril: I’m going to tell you a secret about everyone else’s job. No one knows what they’re doing. I don’t know how to run an animal control department. Half the documents I get I put right into the shredder because they are so boring.\nAndy: But you seem like you do know what you’re doing.\nApril: Yea I seem like it. Deep down everyone is just faking it until they figure it out. And you will too, because you are awesome, and everyone else sucks.\n\n\nIn episode 12, season 6, Ann Perkins is pregnant. Her super optimistic boyfriend, Chris Traeger, is set on fixing all of her problems. But Chris remains puzzled because the more effort he makes to help Ann the more frustrated and upset she becomes. At one point she takes over the Park department’s Wine and Cheese club, a regular excuse to discuss office grievances, to air her own laundry list of complaints. It is with this backdrop that Tom Haverford gives Chris, and all of us, the key management gem:\n\nThe next time Ann complains, just look her in the eyes, nod your head, and say “that sucks”.\n\nThroughout my career I’ve sat in on countless 1:1s, both as a mentor, a manager, and as a managee. These meetings can be quite fruitful, but they can also be a disaster. It takes a special tact and art to master the 1:1, and I believe this advice is a critical component.\nWhy? Because often in 1:1s, someone has something they need to get off their chest. A great manager can take that energy and absorb it. They don’t amplify, they don’t ignore, and critically, they don’t always try to fix. This can be very challenging. Good managers enjoy fixing problems. But sometimes the best thing to do is just “look them in the eyes, nod your head, and say ‘that sucks’”.\nOne final tip. How do you know if someone actually is in need of help? What if they are teetering on the edge of crashing and burn out? I’ve found a simple strategy is to just ask: “Do you want me to try and help solve this problem?”. You may be surprised at how often the answer is no - at least I found it surprising. Surprising until, that is, I turned the tables and thought about how frequently I also just wanted sympathy not solutions.\n\n\n\n",
    "preview": "posts/2020-10-26-parks-and-rec-management-tips/images/paste-F9D9E584.png",
    "last_modified": "2024-04-23T20:22:42-06:00",
    "input_file": {},
    "preview_width": 910,
    "preview_height": 1282
  },
  {
    "path": "posts/2020-10-27-n1/",
    "title": "n=1?",
    "description": "A post about bikes.",
    "author": [
      {
        "name": "Sean Lopp",
        "url": {}
      }
    ],
    "date": "2020-07-05",
    "categories": [
      "cycling"
    ],
    "contents": "\nThe saying goes the number of bikes you need is n+1, or 1 more than the number you currently have. But a few years back a colleague of mine proposed a different formula. He suggested that n=1 where that one bike is a “dropper knobby tire cross between a cross bike and an endurance road bike”.\nI, at the time believing n+1, was sold. After convincing my wife, I sought out such a bike and landed on a 2018 aluminum Salsa Warbird which would subsequently be named Chuck.\nChuckFor those of you interested, the original spec sheet is here. The 99 spokes comparison looks like this:\n99 Spokes RankingAt the time I didn’t know about 99 spokes. Now I do, and now so do you! If you are a bike AND data person, the site is an absolute must.\nOver the years Chuck has stayed mostly the same, aside from an upgraded cassette and the addition of tubeless tires. About two years later, was my colleague right? Does n=1?\nFor n=1\nThis bike is fun because it lets you link together rides that otherwise aren’t easy to accomplish. For example, one of my favorite rides involves a 3 mile gravel road, 14 miles of canyon climbing, a quick mile of single track at the top of the canyon, and then 14 rapid miles of descent home. That combination is hard to beat.\nThis bike is also durable. I love the 1x SRAM Apex groupset. It is shocking how much mental effort goes into the debate “Should I switch the front ring or the back? If I do shift the front, how much should I adjust the back”. This 1x system lets you forget all of that. The rear derailleur is easy to adjust, and is simple enough to fix (when you break the derailleur hanger 😥), and it comes with a super handy 🔒 mechanism that will detension the chain making everything easy.\nI hear rumors that some electric shifters also get rid of that thought, but I don’t believe in bikes that need to be charged, even for shifting purposes.\nBut is it too heavy and slow for the road? Is it too stiff for the trails? In short: no. It is fast enough to hang on most roadie group rides, and it turns easy single track back into a challenge.\nThe most damning evidence for this side of the debate: when it came time for my Dad to buy his one bike, I got convinced him to go with the very similar, slightly less aggressive, Salsa Journeyman. I will admit, if you are more like my wife, you may prefer to skip the drop handle bars and go with commuter bars for comfort.\nAgainst n=1\nThe most damning evidence against this case: since purchasing Chuck I have purchased another bike…. meet Charles (a bit more sophisticated, eh?)\nCharlesSo why did I buy Charles? The short answer is because I was planning (and have done) some pretty serious road rides; e.g, I hope to do the Triple Bypass. The longer answer involves Strava PRs and a disdain for the “they have a lighter bike” excuse.\nCharles is a different ride than Chuck. I quite enjoy the transition, switching from Chuck to Charles feels like hoping on a 🚀. And when I’m tired of grinding hills, it is fun to hit the dirt on Chuck. Chuck also takes the brunt of off-weather rides, meaning that Charles stays clean and mostly maintenance free.\nCharles spends a greater part of the winter on a trainer\nSo whether you want to believe n=1 or not, I can highly recommend both bikes. Ride more, talk about riding slightly less.\nPS\nThis post would feel a bit incomplete if I didn’t introduce you to the final member of the fleet: Charlie. Charlie was my high school mtn bike turned college commuter turned emergency backup. Most of Charlie’s time these days is spent like this:\nCharlie",
    "preview": "posts/2020-10-27-n1/images/paste-DD51328B.png",
    "last_modified": "2024-04-23T20:22:42-06:00",
    "input_file": {},
    "preview_width": 2048,
    "preview_height": 1536
  },
  {
    "path": "posts/2020-10-26-how-to-avoid-firedrills/",
    "title": "How to Avoid Firedrills",
    "description": "My take on \"Agile\" Data Science",
    "author": [
      {
        "name": "Sean Lopp",
        "url": {}
      }
    ],
    "date": "2020-04-20",
    "categories": [
      "data_science",
      "management"
    ],
    "contents": "\n\nPost originally published here: https://blog.rstudio.com/2020/04/28/avoid-irrelevancy-and-fire-drills-in-data-science-teams/\nSome practical tips to implementing this advice are developed here: https://rstudio.com/resources/webinars/avoid-dashboard-fatigue/\nBalancing the twin threats of data science development\nData science leaders naturally want to maximize the value their teams deliver to their organization, and that often means helping them navigate between two possible extremes. On the one hand, a team can easily become an expensive R&D department, detached from actual business decisions, slowly chipping away only to end up answering stale questions. On the other hand, teams can be overwhelmed with requests, spending all of their time on labor intensive, manual fire-drills, always creating one more “Just in Time” Powerpoint slide.\nHow do you avoid these threats, of either irrelevancy or constant fire drills? Turns out the answer is pretty straightforward: use iterative, code-based development to share your content early and often, to help overcome the communications gap with your stakeholders.\nData science ecosystems can be complex and full of jargon, so before we dive into specifics let’s consider a similar balancing act. Imagine you are forming a band that wants to share new music with the world. To do so, it is critical to get music out to your fans quickly, to iterate on ideas rapidly. You don’t want to get bogged down in the details of a recording studio on day 1. At the same time, you want to be able to capture and repeat what works - perhaps as sheet music, perhaps as a video, or even as a simple recording.\nShare your data science work early and often\nFor data scientists, the key is creating the right types of outputs so that decision makers can iterate with you on questions and understand your results. Luckily, like a musician, the modern data science team has many ways to share their initial vision:\nThey can quickly create notebooks, through tools like R Markdown or Jupyter, that are driven by reproducible code and can be shared, scheduled, and viewed without your audience needing to understand code.\nThey can build interactive web applications using tools like Shiny, Flask, or Dash to help non-coders test questions and explore data.\nSometimes, data science teams even create APIs, which act as a realistic preview of their final work with a much lower cost of creation.\nSharing early and often enables data science teams to solve impactful problems. For example, perhaps a data scientist is tasked with forecasting sales by county. They might share their initial exploratory analysis sales leadership and tap into their domain expertise to help explain outlier counties. Or imagine a data scientist working to support biologists doing drug discovery research. Instead of responding to hundreds of requests for statistical analysis, the data scientist could build an interactive application to allow biologists to run their own analysis on different inputs and experiments. By sharing the application early and often, the biologist and data scientist can empower each other to complete far more experiments.\nThese types of outputs all share a few characteristics:\nThe outputs are easy to create. The sign that your team has the right set of tools is if a data scientist can create and share an output from scratch in days, not months. They shouldn’t have to learn a new framework or technology stack.\nThe outputs are reproducible. It can be tempting, in a desire to move quickly, to take shortcuts. However, these shortcuts can undermine your work almost immediately. Data scientists are responsible for informing critical decisions with data. This responsibility is serious, and it means results can not exist only on one person’s laptop, or require manual tweaking to recreate. A lack of reproducibility can undermine credibility in the minds of your stakeholders, which may lead them to dismiss or ignore your analyses if the answer conflicts with their intuition.\nFinally, and most importantly: the outputs must be shared. All of these examples: notebooks, interactive apps and dashboards, and even APIs, are geared towards interacting with decision makers as quickly as possible to be sure the right questions are being answered.\nIt’s not just about production\nWe often see data science teams make a common mistake that prevents them from achieving this delicate balancing act. A tempting trap is to focus exclusively on complex tooling oriented towards putting models in production. Because data science teams are trying to strike a balance between repeatability, robustness, and speed, and because they are working with code, they often turn to their software engineering counterparts for guidance on adopting “agile” processes. Unfortunately, many teams end up focusing on the wrong parts of the agile playbook. Instead of copying the concept - rapid iterations towards a useful goal - teams get caught up in the technologies, introducing complex workflows instead of focusing on results. This mistake leads to a different version of the expensive R&D department - the band stuck in a recording studio with the wrong song.\nEduardo Arina de la Rubio, head of a large data team at Facebook, lays out an important reminder in his recent talk at rstudio::conf 2020. Data science teams are not machine learning engineers. While growth of the two are related, ML models will ultimately become commoditized, mastered by engineers and available in off-the-shelf offerings. Data scientists, on the other hand, have a broader mandate: to enable critical business decisions. Often, in the teams we work with at RStudio, many projects are resolved and decisions made based on the rapid iteration of an app or a notebook. Only on occasion does the result need to be codified into a model at scale - and usually engineers are involved at that stage.\nTo wrap up, at RStudio we get to interact with hundreds of data science teams of all shapes and sizes from all types of industries. The best of these teams have all mastered the same balancing act: they use powerful tools to help them share results quickly, earning them a fanbase among their business stakeholders and helping their companies make great decisions.\n\n\n",
    "preview": "posts/2020-10-26-how-to-avoid-firedrills/images/firedrill.png",
    "last_modified": "2024-04-23T20:22:42-06:00",
    "input_file": {},
    "preview_width": 768,
    "preview_height": 403
  }
]
